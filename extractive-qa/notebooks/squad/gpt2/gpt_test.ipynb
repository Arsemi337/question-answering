{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFAutoModelForQuestionAnswering,\n",
    "    DefaultDataCollator,\n",
    "    GPT2TokenizerFast\n",
    ")\n",
    "import tensorflow as tf\n",
    "from question_answering.constants import constants\n",
    "from question_answering.custom_models.tf_gpt2_for_question_answering import TFGPT2ForQuestionAnswering\n",
    "from question_answering.utils import core_qa_utils\n",
    "from question_answering.utils.extractive_qa import (\n",
    "    graphs,\n",
    "    model_management,\n",
    "    predictions,\n",
    "    squad_metrics,\n",
    "    pure_metrics,\n",
    "    squad_preprocessing,\n",
    "    medical_preprocessing,\n",
    ")\n",
    "from question_answering.paths import extractive_qa_paths\n",
    "import ipynbname\n",
    "from question_answering.keras_callbacks.time_measure_callback import TimeMeasureCallback\n",
    "import numpy as np\n",
    "from datasets import Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:30:50.062329700Z",
     "start_time": "2023-12-10T16:30:43.923933300Z"
    }
   },
   "id": "53e0ef97adea2e1c"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"  # Change to your desired GPT variant\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_name, add_bos_token = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:54:36.592624700Z",
     "start_time": "2023-12-10T16:54:35.865397Z"
    }
   },
   "id": "84a0d1e96e728bc2"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_bos_token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:54:37.056424300Z",
     "start_time": "2023-12-10T16:54:37.032848900Z"
    }
   },
   "id": "e25dac7f60b47797"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:54:39.868188300Z",
     "start_time": "2023-12-10T16:54:39.854319300Z"
    }
   },
   "id": "dcec6f630d087c22"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: TFAutoModelForQuestionAnswering.\nModel type should be one of AlbertConfig, BertConfig, CamembertConfig, ConvBertConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, FlaubertConfig, FunnelConfig, GPTJConfig, LayoutLMv3Config, LongformerConfig, MobileBertConfig, MPNetConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TFAutoModelForQuestionAnswering\n\u001B[1;32m----> 3\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mTFAutoModelForQuestionAnswering\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgpt2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\question_answering\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:569\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    565\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m    566\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[0;32m    567\u001B[0m         pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39mmodel_args, config\u001B[38;5;241m=\u001B[39mconfig, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mhub_kwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    568\u001B[0m     )\n\u001B[1;32m--> 569\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    570\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    572\u001B[0m )\n",
      "\u001B[1;31mValueError\u001B[0m: Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: TFAutoModelForQuestionAnswering.\nModel type should be one of AlbertConfig, BertConfig, CamembertConfig, ConvBertConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, FlaubertConfig, FunnelConfig, GPTJConfig, LayoutLMv3Config, LongformerConfig, MobileBertConfig, MPNetConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, XLMConfig, XLMRobertaConfig, XLNetConfig."
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"gpt2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T18:28:00.031994600Z",
     "start_time": "2023-12-10T18:27:59.807522200Z"
    }
   },
   "id": "c128649eb3618085"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from question_answering.paths import extractive_qa_paths\n",
    "from question_answering.utils import core_qa_utils\n",
    "\n",
    "raw_train_dataset, raw_test_dataset = core_qa_utils.load_datasets_from_json(\n",
    "    dataset_path=extractive_qa_paths.squad_dataset_dir,\n",
    "    filenames=[\"original_train.json\", \"original_test.json\"],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:54:43.865356100Z",
     "start_time": "2023-12-10T16:54:43.787994600Z"
    }
   },
   "id": "4db524b6227e3dd8"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "train_dataset = raw_train_dataset.select(range(80000))\n",
    "val_dataset = raw_train_dataset.select(range(80000, 87599))\n",
    "test_dataset = raw_test_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:54:44.377642800Z",
     "start_time": "2023-12-10T16:54:44.335473500Z"
    }
   },
   "id": "1a19a746ee47bb22"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "(Dataset({\n     features: ['id', 'title', 'context', 'question', 'answers', 'answer_text', 'answer_start'],\n     num_rows: 80000\n }),\n Dataset({\n     features: ['id', 'title', 'context', 'question', 'answers', 'answer_text', 'answer_start'],\n     num_rows: 7599\n }),\n Dataset({\n     features: ['id', 'title', 'context', 'question', 'answers', 'answer_text', 'answer_start'],\n     num_rows: 10570\n }))"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:54:44.784111300Z",
     "start_time": "2023-12-10T16:54:44.751575300Z"
    }
   },
   "id": "f2bda0e8434f1f35"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.sep_token = '<|sep|>'\n",
    "tokenizer.cls_token = '<|cls|>'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:54:53.860865900Z",
     "start_time": "2023-12-10T16:54:53.820288200Z"
    }
   },
   "id": "7fc3ba4a5cfca271"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "special_tokens_map = {'additional_special_tokens': ['<|sep|>', '<|cls|>', '<|question|>', '<|context|>']}\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_tokens_map)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:54:54.302313200Z",
     "start_time": "2023-12-10T16:54:54.292284400Z"
    }
   },
   "id": "6d03b83659014448"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2TokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'sep_token': '<|sep|>', 'pad_token': '<|endoftext|>', 'cls_token': '<|cls|>', 'additional_special_tokens': ['<|context|>', '<|question|>', '<|cls|>', '<|sep|>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t50257: AddedToken(\"<|context|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50258: AddedToken(\"<|question|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50259: AddedToken(\"<|cls|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t50260: AddedToken(\"<|sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:54:54.994600300Z",
     "start_time": "2023-12-10T16:54:54.977068400Z"
    }
   },
   "id": "708f0894d4b45623"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "sampled_set = train_dataset.select(range(5))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:54:56.007514100Z",
     "start_time": "2023-12-10T16:54:55.990981600Z"
    }
   },
   "id": "34c544833827c08b"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/5 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c464a0d9e8a4304b23cb590191774bc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 72\u001B[0m\n\u001B[0;32m     68\u001B[0m     inputs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend_positions\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m end_positions\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inputs\n\u001B[1;32m---> 72\u001B[0m a \u001B[38;5;241m=\u001B[39m \u001B[43msampled_set\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreprocess_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mremove_columns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msampled_set\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumn_names\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\question_answering\\lib\\site-packages\\datasets\\arrow_dataset.py:592\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    590\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    591\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 592\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    593\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    594\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[0;32m    595\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\question_answering\\lib\\site-packages\\datasets\\arrow_dataset.py:557\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    550\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    551\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[0;32m    552\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[0;32m    553\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[0;32m    554\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[0;32m    555\u001B[0m }\n\u001B[0;32m    556\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 557\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    558\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    559\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\question_answering\\lib\\site-packages\\datasets\\arrow_dataset.py:3097\u001B[0m, in \u001B[0;36mDataset.map\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[0;32m   3090\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3091\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mtqdm(\n\u001B[0;32m   3092\u001B[0m         disable\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mis_progress_bar_enabled(),\n\u001B[0;32m   3093\u001B[0m         unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3094\u001B[0m         total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[0;32m   3095\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3096\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[1;32m-> 3097\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m rank, done, content \u001B[38;5;129;01min\u001B[39;00m Dataset\u001B[38;5;241m.\u001B[39m_map_single(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdataset_kwargs):\n\u001B[0;32m   3098\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[0;32m   3099\u001B[0m                 shards_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\question_answering\\lib\\site-packages\\datasets\\arrow_dataset.py:3474\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[0;32m   3470\u001B[0m indices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[0;32m   3471\u001B[0m     \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m*\u001B[39m(\u001B[38;5;28mslice\u001B[39m(i, i \u001B[38;5;241m+\u001B[39m batch_size)\u001B[38;5;241m.\u001B[39mindices(shard\u001B[38;5;241m.\u001B[39mnum_rows)))\n\u001B[0;32m   3472\u001B[0m )  \u001B[38;5;66;03m# Something simpler?\u001B[39;00m\n\u001B[0;32m   3473\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3474\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[43mapply_function_on_filtered_inputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3475\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3476\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3477\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_same_num_examples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mshard\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlist_indexes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3478\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3479\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3480\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m NumExamplesMismatchError:\n\u001B[0;32m   3481\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DatasetTransformationNotAllowedError(\n\u001B[0;32m   3482\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3483\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\question_answering\\lib\\site-packages\\datasets\\arrow_dataset.py:3353\u001B[0m, in \u001B[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001B[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001B[0m\n\u001B[0;32m   3351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_rank:\n\u001B[0;32m   3352\u001B[0m     additional_args \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (rank,)\n\u001B[1;32m-> 3353\u001B[0m processed_inputs \u001B[38;5;241m=\u001B[39m function(\u001B[38;5;241m*\u001B[39mfn_args, \u001B[38;5;241m*\u001B[39madditional_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_kwargs)\n\u001B[0;32m   3354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, LazyDict):\n\u001B[0;32m   3355\u001B[0m     processed_inputs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m   3356\u001B[0m         k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mkeys_to_format\n\u001B[0;32m   3357\u001B[0m     }\n",
      "Cell \u001B[1;32mIn[26], line 30\u001B[0m, in \u001B[0;36mpreprocess_samples\u001B[1;34m(samples)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, offset \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(offset_mapping):\n\u001B[0;32m     29\u001B[0m     decoded \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(input_ids[i])\n\u001B[1;32m---> 30\u001B[0m     sample_idx \u001B[38;5;241m=\u001B[39m \u001B[43msample_map\u001B[49m[i]\n\u001B[0;32m     31\u001B[0m     answer_start \u001B[38;5;241m=\u001B[39m answer_starts[sample_idx][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     32\u001B[0m     answer_text \u001B[38;5;241m=\u001B[39m answer_texts[sample_idx][\u001B[38;5;241m0\u001B[39m]\n",
      "Cell \u001B[1;32mIn[26], line 30\u001B[0m, in \u001B[0;36mpreprocess_samples\u001B[1;34m(samples)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, offset \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(offset_mapping):\n\u001B[0;32m     29\u001B[0m     decoded \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(input_ids[i])\n\u001B[1;32m---> 30\u001B[0m     sample_idx \u001B[38;5;241m=\u001B[39m \u001B[43msample_map\u001B[49m[i]\n\u001B[0;32m     31\u001B[0m     answer_start \u001B[38;5;241m=\u001B[39m answer_starts[sample_idx][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     32\u001B[0m     answer_text \u001B[38;5;241m=\u001B[39m answer_texts[sample_idx][\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1103\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1061\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\DataSpell\\plugins\\python-ce\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py:169\u001B[0m, in \u001B[0;36mstop\u001B[1;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[0;32m    167\u001B[0m     frame \u001B[38;5;241m=\u001B[39m suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[1;32m--> 169\u001B[0m         \u001B[43mmain_debugger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\DataSpell\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1184\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\DataSpell\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1199\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "\"[Q] question [C] context\"\n",
    "\n",
    "def preprocess_samples(samples):\n",
    "    questions = [f\"<|question|> {q.strip()} \" for q in samples[\"question\"]]\n",
    "    contexts = [f\"<|context|> {c.strip()} {c.strip()} {c.strip()} \" for c in samples[\"context\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "    input_ids = inputs.pop(\"input_ids\")\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answer_starts = samples[\"answer_start\"]\n",
    "    answer_texts = samples[\"answer_text\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        decoded = tokenizer.decode(input_ids[i])\n",
    "        sample_idx = sample_map[i]\n",
    "        answer_start = answer_starts[sample_idx][0]\n",
    "        answer_text = answer_texts[sample_idx][0]\n",
    "        start_char = answer_start\n",
    "        end_char = start_char + len(answer_text)\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if (\n",
    "                offset[context_start][0] > start_char\n",
    "                or offset[context_end][1] < end_char\n",
    "        ):\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end TOKEN positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "\n",
    "a = sampled_set.map(\n",
    "    preprocess_samples, batched=True, remove_columns=sampled_set.column_names\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T17:06:45.893600200Z",
     "start_time": "2023-12-10T16:55:02.921869200Z"
    }
   },
   "id": "3b7b1f06f7ab7bf4"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n    num_rows: 5\n})"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T15:06:02.277596700Z",
     "start_time": "2023-12-10T15:06:02.264493500Z"
    }
   },
   "id": "a9e9662c29a9770c"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(a[0][\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T15:05:50.035636800Z",
     "start_time": "2023-12-10T15:05:50.014614700Z"
    }
   },
   "id": "6f9bfd3ebdc6cc0a"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "[['Saint Bernadette Soubirous'],\n ['a copper statue of Christ'],\n ['the Main Building'],\n ['a Marian place of prayer and reflection'],\n ['a golden statue of the Virgin Mary']]"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_set[\"answer_text\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T15:06:45.505022300Z",
     "start_time": "2023-12-10T15:06:45.486963700Z"
    }
   },
   "id": "f0df57829c347efd"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "'e Main B'"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded[a[0][\"start_positions\"]:a[0][\"end_positions\"] + 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T15:06:18.844169600Z",
     "start_time": "2023-12-10T15:06:18.830664700Z"
    }
   },
   "id": "773ba269938ad3f5"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from question_answering.utils.extractive_qa import squad_preprocessing\n",
    "\n",
    "tokenized_train_dataset = squad_preprocessing.preprocess_squad_training_dataset(\n",
    "    dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    stride=stride,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "tokenized_val_dataset = squad_preprocessing.preprocess_squad_training_dataset(\n",
    "    dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    stride=stride,\n",
    "    remove_columns=val_dataset.column_names,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T19:38:53.011474100Z",
     "start_time": "2023-12-08T19:38:52.366714800Z"
    }
   },
   "id": "e12ca472420057c8"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "tokenized_test_dataset = squad_preprocessing.preprocess_squad_test_dataset(\n",
    "    dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=max_length,\n",
    "    stride=stride,\n",
    "    remove_columns=test_dataset.column_names,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T19:38:54.893591900Z",
     "start_time": "2023-12-08T19:38:54.741969600Z"
    }
   },
   "id": "5149704fa70c2166"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from question_answering.constants import constants\n",
    "import ipynbname\n",
    "\n",
    "# Model parameters\n",
    "full_model_name = \"-\".join(ipynbname.name().split(\"_\"))\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint_filename_template = constants.checkpoint_filename_template\n",
    "checkpoints_path = (\n",
    "        extractive_qa_paths.training_checkpoints_dir\n",
    "        / full_model_name\n",
    "        / checkpoint_filename_template\n",
    ")\n",
    "\n",
    "# Saved models\n",
    "saved_models_path = extractive_qa_paths.saved_models_dir / full_model_name\n",
    "\n",
    "# Evaluation\n",
    "model_evaluation_dir = extractive_qa_paths.model_evaluation_dir / full_model_name\n",
    "figures_dir = model_evaluation_dir / constants.figures_dir_name\n",
    "\n",
    "# Hyper parameters\n",
    "batch_size = 4\n",
    "train_epochs = 10\n",
    "initial_learning_rate = 2e-5\n",
    "end_learning_rate = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T19:39:03.359771800Z",
     "start_time": "2023-12-08T19:39:03.285437100Z"
    }
   },
   "id": "61bb9527413c8eec"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "tf_train_dataset = core_qa_utils.convert_to_tf_dataset(\n",
    "    hf_dataset=tokenized_train_dataset,\n",
    "    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"start_positions\", \"end_positions\"],\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "tf_val_dataset = core_qa_utils.convert_to_tf_dataset(\n",
    "    hf_dataset=tokenized_val_dataset,\n",
    "    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"start_positions\", \"end_positions\"],\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "tf_test_dataset = core_qa_utils.convert_to_tf_dataset(\n",
    "    hf_dataset=tokenized_test_dataset,\n",
    "    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n",
    "    label_cols=None,\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58595e44407bd403"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
