{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained on contaxts being original_code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:43:01.672130100Z",
     "start_time": "2023-11-06T17:42:50.404300500Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BartTokenizerFast,\n",
    "    TFAutoModelForQuestionAnswering,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    keras_callbacks,\n",
    "    TFAutoModelForSeq2SeqLM,\n",
    "    TFEncoderDecoderModel,\n",
    ")\n",
    "import tensorflow as tf\n",
    "from huggingface_hub import notebook_login\n",
    "from question_answering.constants import constants\n",
    "from question_answering.utils import core_qa_utils, generative_qa_utils\n",
    "from question_answering.paths import generative_qa_paths\n",
    "from question_answering.keras_callbacks.time_measure_callback import TimeMeasureCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = core_qa_utils.load_train_val_test_datasets(\n",
    "    generative_qa_paths.python_dataset_dir\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = core_qa_utils.convert_dataframes_to_datasets(\n",
    "    [df_train, df_val, df_test]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ceabc25bf3f4828a43b4172aa0bc53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Artur\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d27eaa3fe03481f8c1f0149aaa49c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55b97f6444a4c5b8ad508ceeed1b2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a599d15335743aa8080adf6119e4c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_checkpoint = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizerFast.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartTokenizerFast(name_or_path='facebook/bart-base', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8b08cef84f4ef7a8c8d60c4f61ad2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1490 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc5ea427594471e9f38777cebc927e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803bafbb483143cfa5883bedcf9d7f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens in tokenized train dataset:  9244\n",
      "Max number of tokens in tokenized val dataset:  575\n",
      "Max number of tokens in tokenized test dataset:  901\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sample(sample, max_tokens=None, padding=False):\n",
    "    question = sample[\"questions\"].strip()\n",
    "    context = sample[\"original_code\"].strip()\n",
    "\n",
    "    return tokenizer(question, context, max_length=max_tokens, padding=padding)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_sample)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_sample)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_sample)\n",
    "\n",
    "print(\n",
    "    \"Max number of tokens in tokenized train dataset: \",\n",
    "    len(max(tokenized_train_dataset[\"input_ids\"], key=len)),\n",
    ")\n",
    "print(\n",
    "    \"Max number of tokens in tokenized val dataset: \",\n",
    "    len(max(tokenized_val_dataset[\"input_ids\"], key=len)),\n",
    ")\n",
    "print(\n",
    "    \"Max number of tokens in tokenized test dataset: \",\n",
    "    len(max(tokenized_test_dataset[\"input_ids\"], key=len)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=<generator object filter_samples_below_number_of_tokens.<locals>.<genexpr> at 0x00000293DF7FDB60> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "max_length = 256\n",
    "\n",
    "\n",
    "def filter_samples_below_number_of_tokens(dataset, max_tokens: int):\n",
    "    indices_to_remove = []\n",
    "\n",
    "    # Find indices of samples where number of tokens exceeds max number of tokens\n",
    "    for index, sample in enumerate(dataset):\n",
    "        tokenized_sample = tokenize_sample(sample)\n",
    "        if len(tokenized_sample[\"input_ids\"]) > max_tokens:\n",
    "            indices_to_remove.append(index)\n",
    "\n",
    "    # Keep only samples with number of tokens less or equal than max number of tokens\n",
    "    dataset_indices = range(len(dataset))\n",
    "    filtered_dataset = dataset.select(\n",
    "        index for index in dataset_indices if index not in set(indices_to_remove)\n",
    "    )\n",
    "\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "filtered_train_dataset = filter_samples_below_number_of_tokens(\n",
    "    train_dataset, max_tokens=max_length\n",
    ")\n",
    "filtered_val_dataset = filter_samples_below_number_of_tokens(\n",
    "    val_dataset, max_tokens=max_length\n",
    ")\n",
    "filtered_test_dataset = filter_samples_below_number_of_tokens(\n",
    "    test_dataset, max_tokens=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in tokenized train dataset before filtering:  56080\n",
      "Number of samples in tokenized val dataset before filtering:  7000\n",
      "Number of samples in tokenized test dataset before filtering:  7000\n",
      "\n",
      "---------------\n",
      "\n",
      "Number of samples in tokenized train dataset after filtering:  54930\n",
      "Number of samples in tokenized val dataset after filtering:  6828\n",
      "Number of samples in tokenized test dataset after filtering:  6854\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Number of samples in tokenized train dataset before filtering: \",\n",
    "    len(train_dataset),\n",
    ")\n",
    "print(\"Number of samples in tokenized val dataset before filtering: \", len(val_dataset))\n",
    "print(\n",
    "    \"Number of samples in tokenized test dataset before filtering: \", len(test_dataset)\n",
    ")\n",
    "\n",
    "print(\"\\n---------------\\n\")\n",
    "\n",
    "print(\n",
    "    \"Number of samples in tokenized train dataset after filtering: \",\n",
    "    len(filtered_train_dataset),\n",
    ")\n",
    "print(\n",
    "    \"Number of samples in tokenized val dataset after filtering: \",\n",
    "    len(filtered_val_dataset),\n",
    ")\n",
    "print(\n",
    "    \"Number of samples in tokenized test dataset after filtering: \",\n",
    "    len(filtered_test_dataset),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    questions = [q.strip() for q in dataset[\"questions\"]]\n",
    "    contexts = [c.strip() for c in dataset[\"original_code\"]]\n",
    "    answers = [c.strip() for c in dataset[\"answers\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        # question_context,\n",
    "        text_target=answers,\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "        # padding=\"max_length\",\n",
    "        # return_offsets_mapping=True,\n",
    "    )\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e15c18a0b04424a9f22b25bd9ff81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54930 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b89ccf611fd4393830eb711f4d5648a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6828 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3a4b979be34c338baff04d315d979e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = filtered_train_dataset.map(\n",
    "    preprocess_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=filtered_train_dataset.column_names,\n",
    ")\n",
    "tokenized_val_dataset = filtered_val_dataset.map(\n",
    "    preprocess_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=filtered_val_dataset.column_names,\n",
    ")\n",
    "tokenized_test_dataset = filtered_test_dataset.map(\n",
    "    preprocess_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=filtered_test_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokenized train dataset entries have 256 tokens:  False\n",
      "All tokenized val dataset entries have 256 tokens:  False\n",
      "All tokenized test dataset entries have 256 tokens:  False\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"All tokenized train dataset entries have {max_length} tokens: \",\n",
    "    all(\n",
    "        [\n",
    "            len(input_ids) == max_length\n",
    "            for input_ids in tokenized_train_dataset[\"input_ids\"]\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    f\"All tokenized val dataset entries have {max_length} tokens: \",\n",
    "    all(\n",
    "        [\n",
    "            len(input_ids) == max_length\n",
    "            for input_ids in tokenized_val_dataset[\"input_ids\"]\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    f\"All tokenized test dataset entries have {max_length} tokens: \",\n",
    "    all(\n",
    "        [\n",
    "            len(input_ids) == max_length\n",
    "            for input_ids in tokenized_test_dataset[\"input_ids\"]\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'question_answering.paths.generative_qa_paths' has no attribute 'model_evaluation_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\STUDIA\\IPS\\question-answering\\generative-qa\\notebooks\\python\\python_bart-uncased_1 copy_Karol.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1%20copy_Karol.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m saved_models_path \u001b[39m=\u001b[39m generative_qa_paths\u001b[39m.\u001b[39msaved_models_dir \u001b[39m/\u001b[39m full_model_name\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1%20copy_Karol.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Evaluation\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1%20copy_Karol.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m model_evaluation_dir \u001b[39m=\u001b[39m generative_qa_paths\u001b[39m.\u001b[39;49mmodel_evaluation_dir \u001b[39m/\u001b[39m full_model_name\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1%20copy_Karol.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Hyperparameters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1%20copy_Karol.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'question_answering.paths.generative_qa_paths' has no attribute 'model_evaluation_dir'"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "training_number = 5\n",
    "\n",
    "model_name = \"python-bart-uncased\"\n",
    "full_model_name = f\"{model_name}-{training_number}\"\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint_filename_template = constants.checkpoint_filename_template\n",
    "checkpoints_path = (\n",
    "    generative_qa_paths.training_checkpoints_dir\n",
    "    / full_model_name\n",
    "    / checkpoint_filename_template\n",
    ")\n",
    "\n",
    "# Hub\n",
    "hub_path = generative_qa_paths.hub_models_location / full_model_name\n",
    "\n",
    "# Saved models\n",
    "saved_models_path = generative_qa_paths.saved_models_dir / full_model_name\n",
    "\n",
    "# Evaluation\n",
    "model_evaluation_dir = generative_qa_paths.model_evaluation_dir / full_model_name\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "train_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0e5ddc08354c9f8f26e02d19c8614a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForConditionalGeneration: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight']\n",
      "- This IS expected if you are initializing TFBartForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load model for fine-tuning\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# Dataset preparation\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
    "\n",
    "tf_train_dataset = core_qa_utils.prepare_tf_dataset(\n",
    "    model=model,\n",
    "    hf_dataset=tokenized_train_dataset,\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "tf_val_dataset = core_qa_utils.prepare_tf_dataset(\n",
    "    model=model,\n",
    "    hf_dataset=tokenized_val_dataset,\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "tf_test_dataset = core_qa_utils.prepare_tf_dataset(\n",
    "    model=model,\n",
    "    hf_dataset=tokenized_test_dataset,\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-100"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator.label_pad_token_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 4, 6, 3, 5, 6, 3, 4]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelLengths = [len(tokenized_train_dataset[i]['labels']) for i in range(1, 10)]\n",
    "labelLengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 109, 108, 125, 128, 126, 33, 36, 251]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputIdsLengths = [len(tokenized_train_dataset[i]['input_ids']) for i in range(1, 10)]\n",
    "inputIdsLengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_train_dataset[i] for i in range(1, 10)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Yes</s>'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_train_dataset['labels'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6,), dtype=int32, numpy=array([   0, 9904,    2, -100, -100, -100])>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartTokenizerFast(name_or_path='facebook/bart-base', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(251,), dtype=int32, numpy=\n",
       "array([    0, 27847,     5,  3260,   146,    10, 10606, 17487,     2,\n",
       "           2,  9232,  5293, 20689,  1459,  7605,   495, 11726,   385,\n",
       "        6929,  9291, 10606, 13810,  6929,  6929, 10606,  8504,   495,\n",
       "       11726,   385, 10606, 26411,  2072,   671, 10606,     2,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "           1,     1,     1,     1,     1,     1,     1,     1])>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\STUDIA\\IPS\\question-answering\\generative-qa\\notebooks\\python\\python-bart-uncased-5 is already a clone of https://huggingface.co/nlp-polish/python-bart-uncased-5. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoints_path, verbose=1, save_weights_only=True\n",
    ")\n",
    "early_stop_cb = tf.keras.callbacks.EarlyStopping(patience=1)\n",
    "push_to_hub = keras_callbacks.PushToHubCallback(\n",
    "    output_dir=full_model_name, tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "time_measure_cb = TimeMeasureCallback()\n",
    "\n",
    "callbacks = [\n",
    "    checkpoint_cb,\n",
    "    early_stop_cb,\n",
    "    # push_to_hub,\n",
    "    time_measure_cb\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4070, compute capability 8.9\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "num_train_steps = len(tf_train_dataset) * train_epochs\n",
    "\n",
    "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# Compile\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# metrics = [\"accuracy\"]\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "# model.compile(optimizer=optimizer, metrics=metrics)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bart_for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model (TFBartMainLayer)     multiple                  139420416 \n",
      "                                                                 \n",
      " final_logits_bias (BiasLaye  multiple                 50265     \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 139,470,681\n",
      "Trainable params: 139,420,416\n",
      "Non-trainable params: 50,265\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the new data\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_val_dataset,\n",
    "    epochs=train_epochs,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best version of the model\n",
    "best_model, best_epoch = core_qa_utils.get_best_model_from_checkpoints(\n",
    "    model, history, model_name=full_model_name, remove_checkpoints=True, model_type=\"generative\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model's weights\n",
    "generative_qa_utils.save_model(best_model, model_name=full_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_weights_model = generative_qa_utils.load_weights_into_model(\n",
    "    model=model, \n",
    "    model_name=full_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857/857 [==============================] - 72s 84ms/step - loss: 2.7252\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from the best model\n",
    "loaded_model_evaluation = loaded_weights_model.evaluate(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Artur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Artur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Artur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "sacrebleu_metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_with_xla(batch):\n",
    "    return loaded_weights_model.generate(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        max_new_tokens=max_length,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_dataset2 = tokenized_test_dataset.train_test_split(test_size=0.001)['test']\n",
    "len(tokenized_test_dataset2[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_test_dataset2 = core_qa_utils.prepare_tf_dataset(\n",
    "    model=loaded_weights_model,\n",
    "    hf_dataset=tokenized_test_dataset2,\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 2264,\n",
       " 473,\n",
       " 5,\n",
       " 3260,\n",
       " 11113,\n",
       " 561,\n",
       " 17487,\n",
       " 2,\n",
       " 2,\n",
       " 9232,\n",
       " 11113,\n",
       " 1215,\n",
       " 33966,\n",
       " 1215,\n",
       " 42274,\n",
       " 3023,\n",
       " 31799,\n",
       " 16,\n",
       " 1215,\n",
       " 45041,\n",
       " 35297,\n",
       " 114,\n",
       " 16,\n",
       " 1215,\n",
       " 45041,\n",
       " 3724,\n",
       " 46446,\n",
       " 9624,\n",
       " 8284,\n",
       " 111,\n",
       " 134,\n",
       " 112,\n",
       " 114,\n",
       " 3724,\n",
       " 8061,\n",
       " 321,\n",
       " 775,\n",
       " 48081,\n",
       " 1990,\n",
       " 414,\n",
       " 11,\n",
       " 3023,\n",
       " 414,\n",
       " 46446,\n",
       " 25,\n",
       " 30766,\n",
       " 414,\n",
       " 12313,\n",
       " 3631,\n",
       " 293,\n",
       " 31799,\n",
       " 321,\n",
       " 414,\n",
       " 414,\n",
       " 10975,\n",
       " 111,\n",
       " 134,\n",
       " 27779,\n",
       " 23687,\n",
       " 414,\n",
       " 12313,\n",
       " 3631,\n",
       " 293,\n",
       " 321,\n",
       " 31799,\n",
       " 775,\n",
       " 40462,\n",
       " 414,\n",
       " 671,\n",
       " 46446,\n",
       " 25,\n",
       " 30766,\n",
       " 775,\n",
       " 1493,\n",
       " 671,\n",
       " 46446,\n",
       " 25,\n",
       " 30766,\n",
       " 3023,\n",
       " 1493,\n",
       " 775,\n",
       " 48081,\n",
       " 1990,\n",
       " 414,\n",
       " 11,\n",
       " 3023,\n",
       " 414,\n",
       " 46446,\n",
       " 25,\n",
       " 30766,\n",
       " 414,\n",
       " 12313,\n",
       " 3631,\n",
       " 293,\n",
       " 31799,\n",
       " 321,\n",
       " 414,\n",
       " 414,\n",
       " 10975,\n",
       " 111,\n",
       " 134,\n",
       " 27779,\n",
       " 23687,\n",
       " 414,\n",
       " 12313,\n",
       " 3631,\n",
       " 293,\n",
       " 321,\n",
       " 31799,\n",
       " 775,\n",
       " 40462,\n",
       " 414,\n",
       " 671,\n",
       " 46446,\n",
       " 25,\n",
       " 30766,\n",
       " 775,\n",
       " 2]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_dataset2[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>What does the code flip together?</s></s>def flip_axis_multi x axis is_random False if is_random factor np random uniform -1 1 if factor > 0 results []for data in x data np asarray data swapaxes axis 0 data data[ -1 ]data data swapaxes 0 axis results append data return np asarray results else return np asarray x else results []for data in x data np asarray data swapaxes axis 0 data data[ -1 ]data data swapaxes 0 axis results append data return np asarray results</s>'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_test_dataset2[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.26s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for batch, labels in tqdm(tf_test_dataset2):\n",
    "    predictions = generate_with_xla(batch)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = labels.numpy()\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "    all_preds.extend(decoded_preds)\n",
    "    all_labels.extend(decoded_labels)\n",
    "\n",
    "    data = {\n",
    "        'question_contexts': tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True),\n",
    "        'labels': decoded_labels,\n",
    "        'preds': decoded_preds\n",
    "    }\n",
    "    df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['two arrays',\n",
       "  'No',\n",
       "  'a unique path',\n",
       "  'the digital ocean switch platform',\n",
       "  'the current windowlength',\n",
       "  'a model',\n",
       "  'a simple paginator'],\n",
       " ['the axises of multiple images',\n",
       "  'No',\n",
       "  'a version of path',\n",
       "  'the digital ocean droplet switch',\n",
       "  'small or incorrect window lengths',\n",
       "  'a model instances parameter array',\n",
       "  'a simplepaginator page'])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "contexts = []\n",
    "\n",
    "if 'question_contexts' in df:\n",
    "    for index, row in df.iterrows():\n",
    "        questions.append(row['question_contexts'].split('?')[0] + '?')\n",
    "        contexts.append(row['question_contexts'].split('?')[1])\n",
    "\n",
    "    data = {\n",
    "        'questions': questions,\n",
    "        'contexts': contexts,\n",
    "        'labels': df['labels'],\n",
    "        'preds': df['preds']\n",
    "    }\n",
    "    df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>contexts</th>\n",
       "      <th>labels</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does the code flip together?</td>\n",
       "      <td>def flip_axis_multi x axis is_random False if ...</td>\n",
       "      <td>the axises of multiple images</td>\n",
       "      <td>two arrays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Does circular references between an event on a...</td>\n",
       "      <td>@skip'silverlight' def test_event_lifetime def...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does not exist on the filesystem?</td>\n",
       "      <td>def unique_path path if not os path exists sys...</td>\n",
       "      <td>a version of path</td>\n",
       "      <td>a unique path</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does the code setup?</td>\n",
       "      <td>def setup_platform hass config add_devices dis...</td>\n",
       "      <td>the digital ocean droplet switch</td>\n",
       "      <td>the digital ocean switch platform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What does the code handle?</td>\n",
       "      <td>def _len_guards M if int M M or M &lt; 0 raise Va...</td>\n",
       "      <td>small or incorrect window lengths</td>\n",
       "      <td>the current windowlength</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What does the code convert to an array that ca...</td>\n",
       "      <td>def _model_to_fit_params model fitparam_indice...</td>\n",
       "      <td>a model instances parameter array</td>\n",
       "      <td>a model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What does the code get?</td>\n",
       "      <td>def simple_paginate request queryset per_page ...</td>\n",
       "      <td>a simplepaginator page</td>\n",
       "      <td>a simple paginator</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           questions  \\\n",
       "0                  What does the code flip together?   \n",
       "1  Does circular references between an event on a...   \n",
       "2             What does not exist on the filesystem?   \n",
       "3                          What does the code setup?   \n",
       "4                         What does the code handle?   \n",
       "5  What does the code convert to an array that ca...   \n",
       "6                            What does the code get?   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  def flip_axis_multi x axis is_random False if ...   \n",
       "1  @skip'silverlight' def test_event_lifetime def...   \n",
       "2  def unique_path path if not os path exists sys...   \n",
       "3  def setup_platform hass config add_devices dis...   \n",
       "4  def _len_guards M if int M M or M < 0 raise Va...   \n",
       "5  def _model_to_fit_params model fitparam_indice...   \n",
       "6  def simple_paginate request queryset per_page ...   \n",
       "\n",
       "                              labels                              preds  \n",
       "0      the axises of multiple images                         two arrays  \n",
       "1                                 No                                 No  \n",
       "2                  a version of path                      a unique path  \n",
       "3   the digital ocean droplet switch  the digital ocean switch platform  \n",
       "4  small or incorrect window lengths           the current windowlength  \n",
       "5  a model instances parameter array                            a model  \n",
       "6             a simplepaginator page                 a simple paginator  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0,\n",
       " 'precisions': [0.5263157894736842, 0.25, 0.16666666666666666, 0.0],\n",
       " 'brevity_penalty': 0.6227038648477501,\n",
       " 'length_ratio': 0.6785714285714286,\n",
       " 'translation_length': 19,\n",
       " 'reference_length': 28}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_result = bleu_metric.compute(predictions=all_preds, references=all_labels)\n",
    "bleu_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.46734693877551026,\n",
       " 'rouge2': 0.1285714285714286,\n",
       " 'rougeL': 0.4625850340136055,\n",
       " 'rougeLsum': 0.4680272108843538}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rogue_result = rouge_metric.compute(predictions=all_preds, references=all_labels)\n",
    "rogue_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meteor': 0.2960018704699556}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteor_result = meteor_metric.compute(predictions=all_preds, references=all_labels)\n",
    "meteor_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee859419cb764c0f98883640a0b9dab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36c9ebafefb4eb482ff18a80c4f684b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3ebd6815b7447592dea7222289d404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c272bbb95a4a7d908417bd14eab27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8938b23e94134f86aa0d8fbe96b9d590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': [0.8852345943450928,\n",
       "  1.000000238418579,\n",
       "  0.8785045146942139,\n",
       "  0.952263593673706,\n",
       "  0.8838106393814087,\n",
       "  0.9287854433059692,\n",
       "  0.9260398745536804],\n",
       " 'recall': [0.8251450061798096,\n",
       "  1.000000238418579,\n",
       "  0.8626241087913513,\n",
       "  0.9275587201118469,\n",
       "  0.8552083969116211,\n",
       "  0.8669842481613159,\n",
       "  0.8938043117523193],\n",
       " 'f1': [0.854134202003479,\n",
       "  1.000000238418579,\n",
       "  0.8704918622970581,\n",
       "  0.9397488236427307,\n",
       "  0.8692743182182312,\n",
       "  0.8968214392662048,\n",
       "  0.9096365571022034],\n",
       " 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.35.0)'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertscore_result = bertscore_metric.compute(predictions=all_preds, references=all_labels, lang='en')\n",
    "bertscore_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 16.94436713288991,\n",
       " 'counts': [10, 3, 1, 0],\n",
       " 'totals': [19, 12, 6, 2],\n",
       " 'precisions': [52.63157894736842, 25.0, 16.666666666666668, 25.0],\n",
       " 'bp': 0.6227038648477501,\n",
       " 'sys_len': 19,\n",
       " 'ref_len': 28}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sacrebleu_result = sacrebleu_metric.compute(predictions=all_preds, references=all_labels)\n",
    "sacrebleu_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyprojroot import find_root, has_dir\n",
    "\n",
    "root = find_root(has_dir(\".git\"))\n",
    "generative_qa_dir = root / \"generative-qa\"\n",
    "model_evaluation_dir = generative_qa_dir / \"model-evaluation\" / full_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('e:/STUDIA/IPS/question-answering/generative-qa/model-evaluation/python-bart-uncased-5')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_evaluation_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all relevant training and evaluation metrics to a json file.\n",
    "evaluation_data = {\n",
    "    \"training\": {\n",
    "        \"metrics\": 'history.history',\n",
    "        \"attempted_epochs\": train_epochs,\n",
    "        \"best_epoch\": 'best_epoch',\n",
    "        \"training_time\": \"time_measure_cb.total_training_time()\",\n",
    "        \"gpu\": core_qa_utils.get_gpu_name(),\n",
    "    },\n",
    "    \"test_set\": {\n",
    "        \"loss\": loaded_model_evaluation,\n",
    "        \"bleu\": bleu_result,\n",
    "        \"rogue\": rogue_result,\n",
    "        \"meteor\": meteor_result,\n",
    "        \"bertscore\": bertscore_result,\n",
    "        \"sacrebleu\": sacrebleu_result,\n",
    "    },\n",
    "}\n",
    "\n",
    "core_qa_utils.save_dict_as_json(\n",
    "    evaluation_data, dir_path=model_evaluation_dir, filename=\"evaluation_data.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>contexts</th>\n",
       "      <th>labels</th>\n",
       "      <th>preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does the code get?</td>\n",
       "      <td>def simple_paginate request queryset per_page ...</td>\n",
       "      <td>a simplepaginator page</td>\n",
       "      <td>a simple paginator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does the code handle?</td>\n",
       "      <td>def _len_guards M if int M M or M &lt; 0 raise Va...</td>\n",
       "      <td>small or incorrect window lengths</td>\n",
       "      <td>the current windowlength</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What does the code setup?</td>\n",
       "      <td>def setup_platform hass config add_devices dis...</td>\n",
       "      <td>the digital ocean droplet switch</td>\n",
       "      <td>the digital ocean switch platform</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    questions  \\\n",
       "0     What does the code get?   \n",
       "1  What does the code handle?   \n",
       "2   What does the code setup?   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  def simple_paginate request queryset per_page ...   \n",
       "1  def _len_guards M if int M M or M < 0 raise Va...   \n",
       "2  def setup_platform hass config add_devices dis...   \n",
       "\n",
       "                              labels                              preds  \n",
       "0             a simplepaginator page                 a simple paginator  \n",
       "1  small or incorrect window lengths           the current windowlength  \n",
       "2   the digital ocean droplet switch  the digital ocean switch platform  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_for_manual_check = 3\n",
    "\n",
    "predictions_for_manual_check = df.sample(n = samples_for_manual_check).reset_index(drop=True)\n",
    "predictions_for_manual_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\n",
    "    model_evaluation_dir / \"test_set_sample_generation.csv\", index=True, index_label=\"index\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question_answering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
