{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained on contaxts being original_code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:18:35.738935Z",
     "start_time": "2023-11-22T23:18:26.117466700Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    keras_callbacks,\n",
    "    TFAutoModelForSeq2SeqLM,\n",
    ")\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import concatenate_datasets\n",
    "from question_answering.constants import constants\n",
    "from question_answering.utils import core_qa_utils, generative_qa_utils\n",
    "from question_answering.paths import generative_qa_paths\n",
    "from question_answering.keras_callbacks.time_measure_callback import TimeMeasureCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:18:36.525259800Z",
     "start_time": "2023-11-22T23:18:35.738935Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = core_qa_utils.load_datasets_from_csv(\n",
    "    generative_qa_paths.python_dataset_dir\n",
    ")\n",
    "\n",
    "df_train = pd.concat([df_train, df_val], ignore_index=True)\n",
    "\n",
    "train_dataset, test_dataset = core_qa_utils.convert_dataframes_to_datasets(\n",
    "    [df_train, df_test]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:18:44.876571100Z",
     "start_time": "2023-11-22T23:18:43.766612100Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:19:31.220919Z",
     "start_time": "2023-11-22T23:18:45.884916100Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_sample(sample, max_tokens=None, padding=False):\n",
    "    question = sample[\"questions\"].strip()\n",
    "    context = sample[\"original_code\"].strip()\n",
    "\n",
    "    return tokenizer(question, context, max_length=max_tokens, padding=padding)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_sample)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_sample)\n",
    "\n",
    "print(\n",
    "    \"Max number of tokens in tokenized train dataset: \",\n",
    "    len(max(tokenized_train_dataset[\"input_ids\"], key=len)),\n",
    ")\n",
    "print(\n",
    "    \"Max number of tokens in tokenized test dataset: \",\n",
    "    len(max(tokenized_test_dataset[\"input_ids\"], key=len)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:20:04.290690200Z",
     "start_time": "2023-11-22T23:19:31.220919Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "\n",
    "def filter_samples_below_number_of_tokens(dataset, max_tokens: int):\n",
    "    indices_to_remove = []\n",
    "\n",
    "    # Find indices of samples where number of tokens exceeds max number of tokens\n",
    "    for index, sample in enumerate(dataset):\n",
    "        tokenized_sample = tokenize_sample(sample)\n",
    "        if len(tokenized_sample[\"input_ids\"]) > max_tokens:\n",
    "            indices_to_remove.append(index)\n",
    "\n",
    "    # Keep only samples with number of tokens less or equal than max number of tokens\n",
    "    dataset_indices = range(len(dataset))\n",
    "    filtered_dataset = dataset.select(\n",
    "        index for index in dataset_indices if index not in set(indices_to_remove)\n",
    "    )\n",
    "\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "filtered_train_dataset = filter_samples_below_number_of_tokens(\n",
    "    train_dataset, max_tokens=max_length\n",
    ")\n",
    "filtered_test_dataset = filter_samples_below_number_of_tokens(\n",
    "    test_dataset, max_tokens=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:20:04.306016300Z",
     "start_time": "2023-11-22T23:20:04.290690200Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Number of samples in tokenized train dataset before filtering: \",\n",
    "    len(train_dataset),\n",
    ")\n",
    "print(\n",
    "    \"Number of samples in tokenized test dataset before filtering: \", len(test_dataset)\n",
    ")\n",
    "\n",
    "print(\"\\n---------------\\n\")\n",
    "\n",
    "print(\n",
    "    \"Number of samples in tokenized train dataset after filtering: \",\n",
    "    len(filtered_train_dataset),\n",
    ")\n",
    "print(\n",
    "    \"Number of samples in tokenized test dataset after filtering: \",\n",
    "    len(filtered_test_dataset),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:20:04.368998200Z",
     "start_time": "2023-11-22T23:20:04.306016300Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    questions = [q.strip() for q in dataset[\"questions\"]]\n",
    "    contexts = [c.strip() for c in dataset[\"original_code\"]]\n",
    "    answers = [c.strip() for c in dataset[\"answers\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        text_target=answers,\n",
    "        max_length=max_length,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:20:19.357521400Z",
     "start_time": "2023-11-22T23:20:04.353377900Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_train_dataset = filtered_train_dataset.map(\n",
    "    preprocess_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=filtered_train_dataset.column_names,\n",
    ")\n",
    "tokenized_test_dataset = filtered_test_dataset.map(\n",
    "    preprocess_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=filtered_test_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:20:19.382423200Z",
     "start_time": "2023-11-22T23:20:19.357521400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "training_number = 1\n",
    "\n",
    "model_name = \"python-t5-original\"\n",
    "full_model_name = f\"{model_name}-{training_number}\"\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint_filename_template = constants.checkpoint_filename_template\n",
    "checkpoints_path = (\n",
    "    generative_qa_paths.training_checkpoints_dir\n",
    "    / full_model_name\n",
    "    / checkpoint_filename_template\n",
    ")\n",
    "\n",
    "# Hub\n",
    "hub_path = generative_qa_paths.hub_models_location / full_model_name\n",
    "\n",
    "# Saved models\n",
    "saved_models_path = generative_qa_paths.saved_models_dir / full_model_name\n",
    "\n",
    "# Evaluation\n",
    "model_evaluation_dir = generative_qa_paths.model_evaluation_dir / full_model_name\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "train_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:20:29.980365800Z",
     "start_time": "2023-11-22T23:20:19.380423Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load model for fine-tuning\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:20:31.369012800Z",
     "start_time": "2023-11-22T23:20:29.996021500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset preparation\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
    "\n",
    "tf_train_dataset = core_qa_utils.prepare_tf_dataset(\n",
    "    model=model,\n",
    "    hf_dataset=tokenized_train_dataset,\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "tf_test_dataset = core_qa_utils.prepare_tf_dataset(\n",
    "    model=model,\n",
    "    hf_dataset=tokenized_test_dataset,\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:22:18.790936200Z",
     "start_time": "2023-11-22T23:22:18.775292700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoints_path, verbose=1, save_weights_only=True\n",
    ")\n",
    "# push_to_hub = keras_callbacks.PushToHubCallback(\n",
    "#     output_dir=full_model_name, tokenizer=tokenizer\n",
    "# )\n",
    "\n",
    "time_measure_cb = TimeMeasureCallback()\n",
    "\n",
    "callbacks = [\n",
    "    checkpoint_cb,\n",
    "    # push_to_hub,\n",
    "    time_measure_cb\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:22:19.514518200Z",
     "start_time": "2023-11-22T23:22:19.272315700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "num_train_steps = len(tf_train_dataset) * train_epochs\n",
    "\n",
    "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:22:20.767076800Z",
     "start_time": "2023-11-22T23:22:20.637596100Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the new data\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    epochs=train_epochs,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best version of the model\n",
    "best_model, best_epoch = core_qa_utils.get_best_model_from_checkpoints(\n",
    "    model, history, model_name=full_model_name, metric=\"loss\", remove_checkpoints=True, model_type=\"generative\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model's weights\n",
    "generative_qa_utils.save_model(best_model, model_name=full_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:24:23.686144Z",
     "start_time": "2023-11-22T23:24:13.544955600Z"
    }
   },
   "outputs": [],
   "source": [
    "loaded_weights_model = generative_qa_utils.load_weights_into_model(\n",
    "    model=model, \n",
    "    model_name=full_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T23:22:41.006129700Z",
     "start_time": "2023-11-22T23:22:40.990474400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get predictions from the best model\n",
    "loaded_model_evaluation = loaded_weights_model.evaluate(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_number = 100\n",
    "\n",
    "test_dataset_pandas = tokenized_test_dataset.to_pandas()\n",
    "test_dataset_array = np.array_split(test_dataset_pandas, parts_number)\n",
    "\n",
    "test_datasets_list = []\n",
    "\n",
    "for row in tqdm(test_dataset_array):\n",
    "    test_datasets_list.append(core_qa_utils.prepare_tf_dataset(\n",
    "        model=model,\n",
    "        hf_dataset=Dataset.from_pandas(row),\n",
    "        collator=data_collator,\n",
    "        batch_size=batch_size,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(input_ids=tokenized_test_dataset[0]['input_ids'], \n",
    "               attention_mask=tokenized_test_dataset[0]['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions_and_answers_df = pd.DataFrame()\n",
    "index_to_start_from = 30\n",
    "predictions_list = []\n",
    "labels_list = []\n",
    "question_contexts_list = []\n",
    "\n",
    "i = 0\n",
    "for dataset in tqdm(test_datasets_list):\n",
    "    if i < index_to_start_from:\n",
    "        i = i + 1\n",
    "        continue\n",
    "    for batch, labels in tqdm(dataset):\n",
    "        predictions = generative_qa_utils.generate_predictions(model, batch, max_length)\n",
    "        decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        labels = labels\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        decoded_predictions = [prediction.strip() for prediction in decoded_predictions]\n",
    "        decoded_labels = [label.strip() for label in decoded_labels]\n",
    "        predictions_list.extend(decoded_predictions)\n",
    "        labels_list.extend(decoded_labels)\n",
    "        question_contexts_list.extend(tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True))\n",
    "\n",
    "        data = {\n",
    "            'question_contexts': tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True),\n",
    "            'labels': decoded_labels,\n",
    "            'predictions': decoded_predictions\n",
    "        }\n",
    "        questions_and_answers_df = pd.concat([questions_and_answers_df, pd.DataFrame(data)], ignore_index=True)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dataframe_with_predictions = generative_qa_utils.split_questions_and_contexts_into_two_columns(dataframe=questions_and_answers_df)\n",
    "dataset_dataframe_with_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_result, rogue_result, meteor_result= generative_qa_utils.get_metrics(dataset_dataframe_with_predictions)\n",
    "print(\n",
    "    \"BLEU:\\n\",\n",
    "    bleu_result,\n",
    "    \"\\nROGUE:\\n\",\n",
    "    rogue_result,\n",
    "    \"\\nMETEOR:\\n\",\n",
    "    meteor_result\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_questions_dictionary = generative_qa_utils.get_closed_questions_split_according_to_answer_correctness(dataframe=dataset_dataframe_with_predictions)\n",
    "\n",
    "number_of_closed_questions = len(closed_questions_dictionary['closed_all'])\n",
    "number_of_correct_answers = len(closed_questions_dictionary['closed_correct_answer'])\n",
    "number_of_wrong_answers = len(closed_questions_dictionary['closed_wrong_answer'])\n",
    "\n",
    "number_of_long_answers_for_closed_questions = len(closed_questions_dictionary['closed_long_answer'])\n",
    "\n",
    "print(\"Correct answers: \", number_of_correct_answers)\n",
    "print(\"Wrong answers: \", number_of_wrong_answers)\n",
    "print(\"Long answers for closed questions: \", number_of_long_answers_for_closed_questions)\n",
    "print(\"Wrong answers without the long ones: \", number_of_wrong_answers - number_of_long_answers_for_closed_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all relevant training and evaluation metrics to a json file.\n",
    "evaluation_data = {\n",
    "    \"training\": {\n",
    "        \"metrics\": history.history,\n",
    "        \"attempted_epochs\": train_epochs,\n",
    "        \"best_epoch\": best_epoch,\n",
    "        \"training_time\": time_measure_cb.total_training_time(),\n",
    "        \"gpu\": core_qa_utils.get_gpu_name(),\n",
    "    },\n",
    "    \"test_set\": {\n",
    "        \"loss\": loaded_model_evaluation,\n",
    "        \"bleu\": bleu_result,\n",
    "        \"rogue\": rogue_result,\n",
    "        \"meteor\": meteor_result,\n",
    "        \"closed_questions\": {\n",
    "            \"closed_questions_number\": number_of_closed_questions,\n",
    "            \"correct_answers_number\": number_of_correct_answers,\n",
    "            \"wrong_answers_number\": number_of_wrong_answers,\n",
    "            \"long_answers_for_closed_questions_number\": number_of_long_answers_for_closed_questions,\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "core_qa_utils.save_dict_as_json(\n",
    "    evaluation_data, dir_path=model_evaluation_dir, filename=\"evaluation_data.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dataframe_with_predictions = generative_qa_utils.calculate_bleus_for_each_sample(dataset_dataframe_with_predictions)\n",
    "dataset_dataframe_with_predictions = generative_qa_utils.calculate_rouges_for_each_sample(dataset_dataframe_with_predictions)\n",
    "dataset_dataframe_with_predictions = generative_qa_utils.calculate_meteor_for_each_sample(dataset_dataframe_with_predictions)\n",
    "dataset_dataframe_with_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "prediction_counts_per_metric_range = generative_qa_utils.calculate_prediction_counts_per_metric_range(dataset_dataframe_with_predictions, thresholds)\n",
    "prediction_counts_per_metric_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_qa_utils.plot_prediction_counts_per_metric_range_diagrams(\n",
    "    prediction_counts_per_metric = prediction_counts_per_metric_range,\n",
    "    thresholds=thresholds,\n",
    "    figure_directory_path=model_evaluation_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dataframe_with_predictions.drop(columns=['index']).to_csv(\n",
    "    model_evaluation_dir / \"test_set_sample_generation.csv\", index=True, index_label=\"index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dataframe_with_predictions = pd.read_csv(model_evaluation_dir / \"test_set_sample_generation.csv\")\n",
    "dataset_dataframe_with_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_predictions_and_question_types = generative_qa_utils.add_question_types_to_dataset_dataframe(dataset_predictions_dataframe=dataset_dataframe_with_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_metric_columns_dataframe = dataframe_predictions_and_question_types[['bleu', 'bleu1', 'bleu2', 'rouge1', 'rouge2', 'rougeL', 'meteor', 'question_type']]\n",
    "metric_mean_values_dataframe = only_metric_columns_dataframe.groupby(['question_type']).mean()\n",
    "metric_mean_values_dataframe.insert(\n",
    "    loc=0,\n",
    "    column='question_type',\n",
    "    value=list(only_metric_columns_dataframe.groupby(['question_type']).groups.keys())\n",
    ")\n",
    "metric_mean_values_dataframe.to_csv(\n",
    "    model_evaluation_dir / \"metric_mean_values.csv\", index=False\n",
    ")\n",
    "metric_mean_values_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_qa_utils.plot_prediction_metrics_per_question_type_diagram(\n",
    "    metric_mean_values_dataframe=metric_mean_values_dataframe,\n",
    "    figure_directory_path=model_evaluation_dir / 'figures' / 'metrics-per-type'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_type_metrics_dictionary = generative_qa_utils.count_prediction_numbers_per_metric_range_for_specific_question_type(dataframe_predictions_and_question_types)\n",
    "generative_qa_utils.save_question_type_metrics_dictionary_to_csv(model_evaluation_dir=model_evaluation_dir, question_type_metrics_dictionary=question_type_metrics_dictionary)\n",
    "question_type_metrics_dictionary = generative_qa_utils.read_question_type_metrics_dictionary_from_csv(model_evaluation_dir=model_evaluation_dir)\n",
    "question_type_metrics_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_qa_utils.plot_prediction_counts_per_metric_range_per_question_type_diagram(\n",
    "    question_type_metrics_dictionary=question_type_metrics_dictionary,\n",
    "    figure_directory_path=model_evaluation_dir / 'figures' / 'counts-per-metric-range'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question_answering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
