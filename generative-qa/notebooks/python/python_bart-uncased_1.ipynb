{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained on contaxts being original_code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:43:01.672130100Z",
     "start_time": "2023-11-06T17:42:50.404300500Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFAutoModelForQuestionAnswering,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    keras_callbacks,\n",
    "    TFAutoModelForSeq2SeqLM,\n",
    "    TFEncoderDecoderModel,\n",
    ")\n",
    "import tensorflow as tf\n",
    "from huggingface_hub import notebook_login\n",
    "from question_answering.constants import constants\n",
    "from question_answering.utils import core_qa_utils, generative_qa_utils\n",
    "from question_answering.paths import generative_qa_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = core_qa_utils.load_train_val_test_datasets(\n",
    "    generative_qa_paths.python_dataset_dir\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = core_qa_utils.convert_dataframes_to_datasets(\n",
    "    [df_train, df_val, df_test]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>code</th>\n",
       "      <th>original_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What does the code make ?</td>\n",
       "      <td>a suite</td>\n",
       "      <td>def Make Suite From Dict d label None suite Su...</td>\n",
       "      <td>def MakeSuiteFromDict d label None suite Suite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Does the code make a suite ?</td>\n",
       "      <td>Yes</td>\n",
       "      <td>def Make Suite From Dict d label None suite Su...</td>\n",
       "      <td>def MakeSuiteFromDict d label None suite Suite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Does the code receive a message from a pull su...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>def receive message topic name subscription na...</td>\n",
       "      <td>def receive_message topic_name subscription_na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>What does the code receive from a pull subscri...</td>\n",
       "      <td>a message</td>\n",
       "      <td>def receive message topic name subscription na...</td>\n",
       "      <td>def receive_message topic_name subscription_na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>What be an explicit budget used only ?</td>\n",
       "      <td>to create the campaign</td>\n",
       "      <td>def Create Shared Budget client budget service...</td>\n",
       "      <td>def CreateSharedBudget client budget_service c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56080</th>\n",
       "      <td>56080</td>\n",
       "      <td>What compiles the file filename ?</td>\n",
       "      <td>bytecode</td>\n",
       "      <td>def save pyc filename cfile '%sc' % filename p...</td>\n",
       "      <td>def save_pyc filename cfile '%sc' % filename p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56081</th>\n",
       "      <td>56081</td>\n",
       "      <td>What will this function spawn ?</td>\n",
       "      <td>a thread</td>\n",
       "      <td>def with timeout func args kwargs {} class Res...</td>\n",
       "      <td>def with_timeout func args kwargs {} class Res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56082</th>\n",
       "      <td>56082</td>\n",
       "      <td>What do this function run using the args ?</td>\n",
       "      <td>the given function</td>\n",
       "      <td>def with timeout func args kwargs {} class Res...</td>\n",
       "      <td>def with_timeout func args kwargs {} class Res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56083</th>\n",
       "      <td>56083</td>\n",
       "      <td>How do this function run the given function ?</td>\n",
       "      <td>using the args</td>\n",
       "      <td>def with timeout func args kwargs {} class Res...</td>\n",
       "      <td>def with_timeout func args kwargs {} class Res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56084</th>\n",
       "      <td>56084</td>\n",
       "      <td>What does the code get from the filename or th...</td>\n",
       "      <td>the file</td>\n",
       "      <td>def get Alteration File file Name settings Alt...</td>\n",
       "      <td>def getAlterationFile fileName settingsAlterat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56080 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                          questions  \\\n",
       "0          0                          What does the code make ?   \n",
       "1          1                       Does the code make a suite ?   \n",
       "2          2  Does the code receive a message from a pull su...   \n",
       "3          3  What does the code receive from a pull subscri...   \n",
       "4          4             What be an explicit budget used only ?   \n",
       "...      ...                                                ...   \n",
       "56080  56080                  What compiles the file filename ?   \n",
       "56081  56081                    What will this function spawn ?   \n",
       "56082  56082         What do this function run using the args ?   \n",
       "56083  56083      How do this function run the given function ?   \n",
       "56084  56084  What does the code get from the filename or th...   \n",
       "\n",
       "                      answers  \\\n",
       "0                     a suite   \n",
       "1                         Yes   \n",
       "2                         Yes   \n",
       "3                   a message   \n",
       "4      to create the campaign   \n",
       "...                       ...   \n",
       "56080                bytecode   \n",
       "56081                a thread   \n",
       "56082      the given function   \n",
       "56083          using the args   \n",
       "56084                the file   \n",
       "\n",
       "                                                    code  \\\n",
       "0      def Make Suite From Dict d label None suite Su...   \n",
       "1      def Make Suite From Dict d label None suite Su...   \n",
       "2      def receive message topic name subscription na...   \n",
       "3      def receive message topic name subscription na...   \n",
       "4      def Create Shared Budget client budget service...   \n",
       "...                                                  ...   \n",
       "56080  def save pyc filename cfile '%sc' % filename p...   \n",
       "56081  def with timeout func args kwargs {} class Res...   \n",
       "56082  def with timeout func args kwargs {} class Res...   \n",
       "56083  def with timeout func args kwargs {} class Res...   \n",
       "56084  def get Alteration File file Name settings Alt...   \n",
       "\n",
       "                                           original_code  \n",
       "0      def MakeSuiteFromDict d label None suite Suite...  \n",
       "1      def MakeSuiteFromDict d label None suite Suite...  \n",
       "2      def receive_message topic_name subscription_na...  \n",
       "3      def receive_message topic_name subscription_na...  \n",
       "4      def CreateSharedBudget client budget_service c...  \n",
       "...                                                  ...  \n",
       "56080  def save_pyc filename cfile '%sc' % filename p...  \n",
       "56081  def with_timeout func args kwargs {} class Res...  \n",
       "56082  def with_timeout func args kwargs {} class Res...  \n",
       "56083  def with_timeout func args kwargs {} class Res...  \n",
       "56084  def getAlterationFile fileName settingsAlterat...  \n",
       "\n",
       "[56080 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = \"t5-small\"\n",
    "model_checkpoint = \"facebook/bart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efdc4d0af33b4ba09e799c1aed872f1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1489 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fc5fce12b84b6385021b5200ef966e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac6466f86e248569556cdcf0afdc566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens in tokenized train dataset:  9243\n",
      "Max number of tokens in tokenized val dataset:  574\n",
      "Max number of tokens in tokenized test dataset:  900\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sample(sample, max_tokens=None, padding=False):\n",
    "    question = sample[\"questions\"].strip()\n",
    "    context = sample[\"original_code\"].strip()\n",
    "\n",
    "    question_context = context + ' - ' + question\n",
    "\n",
    "    # return tokenizer(question, context, max_length=max_tokens, padding=padding)\n",
    "    return tokenizer(question_context, max_length=max_tokens, padding=padding)\n",
    "\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_sample)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_sample)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_sample)\n",
    "\n",
    "print(\n",
    "    \"Max number of tokens in tokenized train dataset: \",\n",
    "    len(max(tokenized_train_dataset[\"input_ids\"], key=len)),\n",
    ")\n",
    "print(\n",
    "    \"Max number of tokens in tokenized val dataset: \",\n",
    "    len(max(tokenized_val_dataset[\"input_ids\"], key=len)),\n",
    ")\n",
    "print(\n",
    "    \"Max number of tokens in tokenized test dataset: \",\n",
    "    len(max(tokenized_test_dataset[\"input_ids\"], key=len)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=<generator object filter_samples_below_number_of_tokens.<locals>.<genexpr> at 0x0000023DEA7AF610> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "max_length = 256\n",
    "\n",
    "\n",
    "def filter_samples_below_number_of_tokens(dataset, max_tokens: int):\n",
    "    indices_to_remove = []\n",
    "\n",
    "    # Find indices of samples where number of tokens exceeds max number of tokens\n",
    "    for index, sample in enumerate(dataset):\n",
    "        tokenized_sample = tokenize_sample(sample)\n",
    "        if len(tokenized_sample[\"input_ids\"]) > max_tokens:\n",
    "            indices_to_remove.append(index)\n",
    "\n",
    "    # Keep only samples with number of tokens less or equal than max number of tokens\n",
    "    dataset_indices = range(len(dataset))\n",
    "    filtered_dataset = dataset.select(\n",
    "        index for index in dataset_indices if index not in set(indices_to_remove)\n",
    "    )\n",
    "\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "filtered_train_dataset = filter_samples_below_number_of_tokens(\n",
    "    train_dataset, max_tokens=max_length\n",
    ")\n",
    "filtered_val_dataset = filter_samples_below_number_of_tokens(\n",
    "    val_dataset, max_tokens=max_length\n",
    ")\n",
    "filtered_test_dataset = filter_samples_below_number_of_tokens(\n",
    "    test_dataset, max_tokens=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in tokenized train dataset before filtering:  56080\n",
      "Number of samples in tokenized val dataset before filtering:  7000\n",
      "Number of samples in tokenized test dataset before filtering:  7000\n",
      "\n",
      "---------------\n",
      "\n",
      "Number of samples in tokenized train dataset after filtering:  54958\n",
      "Number of samples in tokenized val dataset after filtering:  6832\n",
      "Number of samples in tokenized test dataset after filtering:  6859\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Number of samples in tokenized train dataset before filtering: \",\n",
    "    len(train_dataset),\n",
    ")\n",
    "print(\"Number of samples in tokenized val dataset before filtering: \", len(val_dataset))\n",
    "print(\n",
    "    \"Number of samples in tokenized test dataset before filtering: \", len(test_dataset)\n",
    ")\n",
    "\n",
    "print(\"\\n---------------\\n\")\n",
    "\n",
    "print(\n",
    "    \"Number of samples in tokenized train dataset after filtering: \",\n",
    "    len(filtered_train_dataset),\n",
    ")\n",
    "print(\n",
    "    \"Number of samples in tokenized val dataset after filtering: \",\n",
    "    len(filtered_val_dataset),\n",
    ")\n",
    "print(\n",
    "    \"Number of samples in tokenized test dataset after filtering: \",\n",
    "    len(filtered_test_dataset),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_question_and_context(dataset_row):\n",
    "    context_question = dataset_row['original_code'] + ' - ' + dataset_row['questions']\n",
    "    dataset_row['context_question'] = context_question\n",
    "    return dataset_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68485e92616d40879a6b248da214b831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54958 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0fc9bfbc0444a19546f96737a5431b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648f7a8a855846c7b5c7490f92e9d734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_train_dataset = filtered_train_dataset.map(combine_question_and_context)\n",
    "filtered_val_dataset = filtered_val_dataset.map(combine_question_and_context)\n",
    "filtered_test_dataset = filtered_test_dataset.map(combine_question_and_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def MakeSuiteFromDict d label None suite Suite label label suite SetDict d suite Normalize return suite - What does the code make ?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_train_dataset['context_question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    # questions = [q.strip() for q in dataset[\"questions\"]]\n",
    "    # contexts = [c.strip() for c in dataset[\"original_code\"]]\n",
    "    answers = [c.strip() for c in dataset[\"answers\"]]\n",
    "\n",
    "    question_context = [cq.strip() for cq in dataset[\"context_question\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        # questions,\n",
    "        # contexts,\n",
    "        question_context,\n",
    "        text_target=answers,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4552c90caa8948b8afa1f7c7a93087b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/54958 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219a9c6e179d4faab86fd89a98a4985a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5463fe6dce4910b556ee68f6605122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = filtered_train_dataset.map(\n",
    "    preprocess_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=filtered_train_dataset.column_names,\n",
    ")\n",
    "tokenized_val_dataset = filtered_val_dataset.map(\n",
    "    preprocess_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=filtered_val_dataset.column_names,\n",
    ")\n",
    "tokenized_test_dataset = filtered_test_dataset.map(\n",
    "    preprocess_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=filtered_test_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 0,\n",
       " 'questions': 'How does the code add a user in the given buckets default object access control list ?',\n",
       " 'answers': 'as an owner',\n",
       " 'code': \"def add bucket default owner bucket name user email storage client storage Client bucket storage client bucket bucket name bucket acl reload bucket default object acl user user email grant owner bucket default object acl save print ' Addeduser{}asanownerinthedefaultaclonbucket{} ' format user email bucket name\",\n",
       " 'original_code': \"def add_bucket_default_owner bucket_name user_email storage_client storage Client bucket storage_client bucket bucket_name bucket acl reload bucket default_object_acl user user_email grant_owner bucket default_object_acl save print 'Addeduser{}asanownerinthedefaultaclonbucket{} ' format user_email bucket_name\",\n",
       " 'context_question': \"def add_bucket_default_owner bucket_name user_email storage_client storage Client bucket storage_client bucket bucket_name bucket acl reload bucket default_object_acl user user_email grant_owner bucket default_object_acl save print 'Addeduser{}asanownerinthedefaultaclonbucket{} ' format user_email bucket_name - How does the code add a user in the given buckets default object access control list ?\"}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokenized train dataset entries have 256 tokens:  True\n",
      "All tokenized val dataset entries have 256 tokens:  True\n",
      "All tokenized test dataset entries have 256 tokens:  True\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"All tokenized train dataset entries have {max_length} tokens: \",\n",
    "    all(\n",
    "        [\n",
    "            len(input_ids) == max_length\n",
    "            for input_ids in tokenized_train_dataset[\"input_ids\"]\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    f\"All tokenized val dataset entries have {max_length} tokens: \",\n",
    "    all(\n",
    "        [\n",
    "            len(input_ids) == max_length\n",
    "            for input_ids in tokenized_val_dataset[\"input_ids\"]\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    f\"All tokenized test dataset entries have {max_length} tokens: \",\n",
    "    all(\n",
    "        [\n",
    "            len(input_ids) == max_length\n",
    "            for input_ids in tokenized_test_dataset[\"input_ids\"]\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "training_number = 2\n",
    "\n",
    "model_name = \"python-bart-uncased\"\n",
    "full_model_name = f\"{model_name}-{training_number}\"\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint_filename_template = constants.checkpoint_filename_template\n",
    "checkpoints_path = (\n",
    "    generative_qa_paths.training_checkpoints_dir\n",
    "    / full_model_name\n",
    "    / checkpoint_filename_template\n",
    ")\n",
    "\n",
    "# Hub\n",
    "hub_path = generative_qa_paths.hub_models_location / full_model_name\n",
    "\n",
    "# Saved models\n",
    "saved_models_path = generative_qa_paths.saved_models_dir / full_model_name\n",
    "\n",
    "# Figures\n",
    "figures_dir = generative_qa_paths.figures_dir / full_model_name\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 8\n",
    "train_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForConditionalGeneration: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "- This IS expected if you are initializing TFBartForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load model for fine-tuning\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# Dataset preparation\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
    "\n",
    "tf_train_dataset = core_qa_utils.prepare_tf_dataset(\n",
    "    model=model,\n",
    "    hf_dataset=tokenized_train_dataset,\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "tf_val_dataset = core_qa_utils.prepare_tf_dataset(\n",
    "    model=model,\n",
    "    hf_dataset=tokenized_val_dataset,\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "tf_test_dataset = core_qa_utils.prepare_tf_dataset(\n",
    "    model=model,\n",
    "    hf_dataset=tokenized_test_dataset,\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoints_path, verbose=1, save_weights_only=True\n",
    ")\n",
    "early_stop_cb = tf.keras.callbacks.EarlyStopping(patience=1)\n",
    "# push_to_hub = keras_callbacks.PushToHubCallback(\n",
    "#     output_dir=full_model_name, tokenizer=tokenizer\n",
    "# )\n",
    "\n",
    "callbacks = [\n",
    "    checkpoint_cb,\n",
    "    early_stop_cb,\n",
    "    # push_to_hub\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4070, compute capability 8.9\n"
     ]
    }
   ],
   "source": [
    "# Compile\n",
    "num_train_steps = len(tf_train_dataset) * train_epochs\n",
    "\n",
    "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# Compile\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# metrics = [\"accuracy\"]\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "# model.compile(optimizer=optimizer, metrics=metrics)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bart_for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model (TFBartMainLayer)     multiple                  139420416 \n",
      "                                                                 \n",
      " final_logits_bias (BiasLaye  multiple                 50265     \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 139,470,681\n",
      "Trainable params: 139,420,416\n",
      "Non-trainable params: 50,265\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6869/6869 [==============================] - ETA: 0s - loss: 2.7180\n",
      "Epoch 1: saving model to e:\\STUDIA\\IPS\\question-answering\\generative-qa\\training-checkpoints\\python-bart-uncased-2\\cp-01.ckpt\n",
      "6869/6869 [==============================] - 1615s 232ms/step - loss: 2.7180 - val_loss: 2.7321\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on the new data\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_val_dataset,\n",
    "    epochs=train_epochs,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best version of the model\n",
    "best_model, best_epoch = core_qa_utils.get_best_model_from_checkpoints(\n",
    "    model, history, model_name=full_model_name, remove_checkpoints=True, model_type=\"generative\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model's weights\n",
    "generative_qa_utils.save_model(best_model, model_name=full_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "loaded_model = generative_qa_utils.load_model(\n",
    "    model_checkpoint, model_name=full_model_name\n",
    ")\n",
    "loaded_model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the best model\n",
    "loaded_model_evaluation = loaded_model.evaluate(tf_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_with_xla(batch):\n",
    "    return loaded_model.generate(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        max_new_tokens=max_length,\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_metrics():\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch, labels in tqdm(tf_test_dataset):\n",
    "        predictions = generate_with_xla(batch)\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        labels = labels.numpy()\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "        all_preds.extend(decoded_preds)\n",
    "        all_labels.extend(decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=all_preds, references=all_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_dataset2 = tokenized_test_dataset.train_test_split(test_size=0.001)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_attention_mask(value):\n",
    "    value = 1 - value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_list = tokenized_test_dataset2[0]['attention_mask']\n",
    "iterator = masks = map(lambda x: 1 - x, original_list)\n",
    "list(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_attention_mask_in_dataset(dataset_row):\n",
    "    something = dataset_row[\"attention_mask\"]\n",
    "    something2 = list(map(lambda x: 1 - x, something))\n",
    "    dataset_row[\"attention_mask\"] = something2\n",
    "    return dataset_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_dataset3 = tokenized_test_dataset2.map(invert_attention_mask_in_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenized_test_dataset2[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenized_test_dataset2[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'labels'],\n",
       "    num_rows: 7\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\STUDIA\\IPS\\question-answering\\generative-qa\\notebooks\\python\\python_bart-uncased_1.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loaded_model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1.ipynb#Y112sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mtokenized_test_dataset2[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1.ipynb#Y112sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mtokenized_test_dataset2[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1.ipynb#Y112sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49mmax_length,)\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\transformers\\generation\\tf_utils.py:791\u001b[0m, in \u001b[0;36mTFGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, seed, **kwargs)\u001b[0m\n\u001b[0;32m    787\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_model_inputs(\n\u001b[0;32m    788\u001b[0m     inputs, generation_config\u001b[39m.\u001b[39mbos_token_id, model_kwargs\n\u001b[0;32m    789\u001b[0m )\n\u001b[0;32m    790\u001b[0m \u001b[39m# inputs_ids now has to be defined and cannot be None anymore\u001b[39;00m\n\u001b[1;32m--> 791\u001b[0m batch_size \u001b[39m=\u001b[39m shape_list(inputs_tensor)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    793\u001b[0m \u001b[39m# 5. Prepare other model kwargs\u001b[39;00m\n\u001b[0;32m    794\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39moutput_attentions\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m generation_config\u001b[39m.\u001b[39moutput_attentions\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\transformers\\tf_utils.py:41\u001b[0m, in \u001b[0;36mshape_list\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(tensor\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     39\u001b[0m dynamic \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mshape(tensor)\n\u001b[1;32m---> 41\u001b[0m \u001b[39mif\u001b[39;00m tensor\u001b[39m.\u001b[39;49mshape \u001b[39m==\u001b[39m tf\u001b[39m.\u001b[39mTensorShape(\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m dynamic\n\u001b[0;32m     44\u001b[0m static \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mas_list()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "loaded_model.generate(\n",
    "    input_ids=tokenized_test_dataset2[0]['input_ids'],\n",
    "    attention_mask=tokenized_test_dataset2[0]['labels'],\n",
    "    max_new_tokens=max_length,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_test_dataset2 = core_qa_utils.prepare_tf_dataset(\n",
    "    model=model,\n",
    "    hf_dataset=tokenized_test_dataset2,\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute AddV2 as input #1(zero-based) was expected to be a half tensor but is a float tensor [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32me:\\STUDIA\\IPS\\question-answering\\generative-qa\\notebooks\\python\\python_bart-uncased_1.ipynb Cell 38\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m all_labels \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, labels \u001b[39min\u001b[39;00m tqdm(tf_test_dataset2):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     predictions \u001b[39m=\u001b[39m generate_with_xla(batch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     decoded_preds \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(predictions, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32me:\\STUDIA\\IPS\\question-answering\\generative-qa\\notebooks\\python\\python_bart-uncased_1.ipynb Cell 38\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_with_xla\u001b[39m(batch):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loaded_model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         input_ids\u001b[39m=\u001b[39;49mbatch[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mbatch[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1.ipynb#X45sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         max_new_tokens\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bart-uncased_1.ipynb#X45sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\transformers\\generation\\tf_utils.py:980\u001b[0m, in \u001b[0;36mTFGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, seed, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m    972\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    973\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    976\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    977\u001b[0m     )\n\u001b[0;32m    979\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[0;32m    981\u001b[0m         input_ids,\n\u001b[0;32m    982\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[0;32m    983\u001b[0m         pad_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mpad_token_id,\n\u001b[0;32m    984\u001b[0m         eos_token_id\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39meos_token_id,\n\u001b[0;32m    985\u001b[0m         length_penalty\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mlength_penalty,\n\u001b[0;32m    986\u001b[0m         early_stopping\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mearly_stopping,\n\u001b[0;32m    987\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m    988\u001b[0m         output_scores\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39moutput_scores,\n\u001b[0;32m    989\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m    990\u001b[0m         num_return_sequences\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[0;32m    991\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    992\u001b[0m     )\n\u001b[0;32m    994\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[0;32m    995\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_beams \u001b[39m<\u001b[39m generation_config\u001b[39m.\u001b[39mnum_return_sequences:\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\transformers\\generation\\tf_utils.py:2589\u001b[0m, in \u001b[0;36mTFGenerationMixin.beam_search\u001b[1;34m(self, input_ids, do_sample, max_length, pad_token_id, eos_token_id, length_penalty, early_stopping, logits_processor, logits_warper, num_return_sequences, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[0;32m   2576\u001b[0m \u001b[39m# 2-to-n generation steps can then be run in autoregressive fashion (only in case 1st generation step does\u001b[39;00m\n\u001b[0;32m   2577\u001b[0m \u001b[39m# NOT yield EOS token though)\u001b[39;00m\n\u001b[0;32m   2578\u001b[0m maximum_iterations \u001b[39m=\u001b[39m max_length \u001b[39m-\u001b[39m cur_len\n\u001b[0;32m   2579\u001b[0m (\n\u001b[0;32m   2580\u001b[0m     cur_len,\n\u001b[0;32m   2581\u001b[0m     running_sequences,\n\u001b[0;32m   2582\u001b[0m     running_scores,\n\u001b[0;32m   2583\u001b[0m     running_beam_indices,\n\u001b[0;32m   2584\u001b[0m     sequences,\n\u001b[0;32m   2585\u001b[0m     scores,\n\u001b[0;32m   2586\u001b[0m     beam_indices,\n\u001b[0;32m   2587\u001b[0m     is_sent_finished,\n\u001b[0;32m   2588\u001b[0m     _,\n\u001b[1;32m-> 2589\u001b[0m ) \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mwhile_loop(\n\u001b[0;32m   2590\u001b[0m     beam_search_cond_fn,\n\u001b[0;32m   2591\u001b[0m     beam_search_body_fn,\n\u001b[0;32m   2592\u001b[0m     (\n\u001b[0;32m   2593\u001b[0m         cur_len,\n\u001b[0;32m   2594\u001b[0m         running_sequences,\n\u001b[0;32m   2595\u001b[0m         running_scores,\n\u001b[0;32m   2596\u001b[0m         running_beam_indices,\n\u001b[0;32m   2597\u001b[0m         sequences,\n\u001b[0;32m   2598\u001b[0m         scores,\n\u001b[0;32m   2599\u001b[0m         beam_indices,\n\u001b[0;32m   2600\u001b[0m         is_sent_finished,\n\u001b[0;32m   2601\u001b[0m         model_kwargs,\n\u001b[0;32m   2602\u001b[0m     ),\n\u001b[0;32m   2603\u001b[0m     maximum_iterations\u001b[39m=\u001b[39;49mmaximum_iterations,\n\u001b[0;32m   2604\u001b[0m )\n\u001b[0;32m   2606\u001b[0m \u001b[39m# 6. prepare outputs\u001b[39;00m\n\u001b[0;32m   2607\u001b[0m \u001b[39m# Account for the edge-case where there are no finished sequences for a particular batch item. If so, return\u001b[39;00m\n\u001b[0;32m   2608\u001b[0m \u001b[39m# running sequences for that batch item.\u001b[39;00m\n\u001b[0;32m   2609\u001b[0m none_finished \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mreduce_any(is_sent_finished, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:629\u001b[0m, in \u001b[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m           _PRINTED_WARNING[(func, arg_name)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    623\u001b[0m         logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    624\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is deprecated and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    625\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mwill be removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m    626\u001b[0m             _call_location(), decorator_utils\u001b[39m.\u001b[39mget_qualified_name(func),\n\u001b[0;32m    627\u001b[0m             func\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m, arg_name, arg_value, \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    628\u001b[0m             \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date), instructions)\n\u001b[1;32m--> 629\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2513\u001b[0m, in \u001b[0;36mwhile_loop_v2\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[0;32m   2337\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mwhile_loop\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m   2338\u001b[0m \u001b[39m@deprecation\u001b[39m\u001b[39m.\u001b[39mdeprecated_arg_values(\n\u001b[0;32m   2339\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2354\u001b[0m                   maximum_iterations\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2355\u001b[0m                   name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   2356\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Repeat `body` while the condition `cond` is true.\u001b[39;00m\n\u001b[0;32m   2357\u001b[0m \n\u001b[0;32m   2358\u001b[0m \u001b[39m  `cond` is a callable returning a boolean scalar tensor. `body` is a callable\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2511\u001b[0m \n\u001b[0;32m   2512\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2513\u001b[0m   \u001b[39mreturn\u001b[39;00m while_loop(\n\u001b[0;32m   2514\u001b[0m       cond\u001b[39m=\u001b[39;49mcond,\n\u001b[0;32m   2515\u001b[0m       body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m   2516\u001b[0m       loop_vars\u001b[39m=\u001b[39;49mloop_vars,\n\u001b[0;32m   2517\u001b[0m       shape_invariants\u001b[39m=\u001b[39;49mshape_invariants,\n\u001b[0;32m   2518\u001b[0m       parallel_iterations\u001b[39m=\u001b[39;49mparallel_iterations,\n\u001b[0;32m   2519\u001b[0m       back_prop\u001b[39m=\u001b[39;49mback_prop,\n\u001b[0;32m   2520\u001b[0m       swap_memory\u001b[39m=\u001b[39;49mswap_memory,\n\u001b[0;32m   2521\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   2522\u001b[0m       maximum_iterations\u001b[39m=\u001b[39;49mmaximum_iterations,\n\u001b[0;32m   2523\u001b[0m       return_same_structure\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2762\u001b[0m, in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   2759\u001b[0m loop_var_structure \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(type_spec\u001b[39m.\u001b[39mtype_spec_from_value,\n\u001b[0;32m   2760\u001b[0m                                         \u001b[39mlist\u001b[39m(loop_vars))\n\u001b[0;32m   2761\u001b[0m \u001b[39mwhile\u001b[39;00m cond(\u001b[39m*\u001b[39mloop_vars):\n\u001b[1;32m-> 2762\u001b[0m   loop_vars \u001b[39m=\u001b[39m body(\u001b[39m*\u001b[39;49mloop_vars)\n\u001b[0;32m   2763\u001b[0m   \u001b[39mif\u001b[39;00m try_to_pack \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(loop_vars, (\u001b[39mlist\u001b[39m, _basetuple)):\n\u001b[0;32m   2764\u001b[0m     packed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2753\u001b[0m, in \u001b[0;36mwhile_loop.<locals>.<lambda>\u001b[1;34m(i, lv)\u001b[0m\n\u001b[0;32m   2750\u001b[0m     loop_vars \u001b[39m=\u001b[39m (counter, loop_vars)\n\u001b[0;32m   2751\u001b[0m     cond \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m i, lv: (  \u001b[39m# pylint: disable=g-long-lambda\u001b[39;00m\n\u001b[0;32m   2752\u001b[0m         math_ops\u001b[39m.\u001b[39mlogical_and(i \u001b[39m<\u001b[39m maximum_iterations, orig_cond(\u001b[39m*\u001b[39mlv)))\n\u001b[1;32m-> 2753\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m i, lv: (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, orig_body(\u001b[39m*\u001b[39;49mlv))\n\u001b[0;32m   2754\u001b[0m   try_to_pack \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   2756\u001b[0m \u001b[39mif\u001b[39;00m executing_eagerly:\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\transformers\\generation\\tf_utils.py:2381\u001b[0m, in \u001b[0;36mTFGenerationMixin.beam_search.<locals>.beam_search_body_fn\u001b[1;34m(cur_len, running_sequences, running_scores, running_beam_indices, sequences, scores, beam_indices, is_sent_finished, model_kwargs)\u001b[0m\n\u001b[0;32m   2379\u001b[0m     log_probs \u001b[39m=\u001b[39m unflatten_beam_dim(log_probs, num_beams)\n\u001b[0;32m   2380\u001b[0m log_probs_processed \u001b[39m=\u001b[39m log_probs\n\u001b[1;32m-> 2381\u001b[0m log_probs \u001b[39m=\u001b[39m log_probs \u001b[39m+\u001b[39;49m tf\u001b[39m.\u001b[39;49mexpand_dims(running_scores, axis\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m   2382\u001b[0m vocab_size \u001b[39m=\u001b[39m log_probs\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n\u001b[0;32m   2383\u001b[0m log_probs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(log_probs, (batch_size, num_beams \u001b[39m*\u001b[39m vocab_size))\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: cannot compute AddV2 as input #1(zero-based) was expected to be a half tensor but is a float tensor [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for batch, labels in tqdm(tf_test_dataset2):\n",
    "    predictions = generate_with_xla(batch)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = labels.numpy()\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    all_preds.extend(decoded_preds)\n",
    "    all_labels.extend(decoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = metric.compute(predictions=all_preds, references=all_labels)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question_answering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
