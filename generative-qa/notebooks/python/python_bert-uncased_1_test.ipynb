{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model trained on contaxts being original_code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:43:01.672130100Z",
     "start_time": "2023-11-06T17:42:50.404300500Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TFAutoModelForQuestionAnswering,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    keras_callbacks,\n",
    "    TFAutoModelForSeq2SeqLM,\n",
    "    TFEncoderDecoderModel,\n",
    ")\n",
    "import tensorflow as tf\n",
    "from huggingface_hub import notebook_login\n",
    "from question_answering.constants import constants\n",
    "from question_answering.utils import core_qa_utils, extractive_qa_utils\n",
    "from question_answering.paths import generative_qa_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = core_qa_utils.load_train_val_test_datasets(\n",
    "    generative_qa_paths.python_dataset_dir\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = core_qa_utils.convert_dataframes_to_datasets(\n",
    "    [df_train, df_val, df_test]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFEncoderDecoderModel.\n",
      "\n",
      "All the layers of TFEncoderDecoderModel were initialized from the model checkpoint at nlp-polish/generative-test.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFEncoderDecoderModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"nlp-polish/generative-test\", pad_token_id=50257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model_checkpoint = \"t5-small\"\n",
    "# model_checkpoint = \"bert-base-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1180d7cea2274ca69e2dd33512bcc411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56080 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592e57c6b64343c9a2478bb99c930f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fb38ff517c4a34b511f5ac38e44013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens in tokenized train dataset:  8980\n",
      "Max number of tokens in tokenized val dataset:  657\n",
      "Max number of tokens in tokenized test dataset:  920\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sample(sample, max_tokens=None, padding=False):\n",
    "    question = sample[\"questions\"].strip()\n",
    "    context = sample[\"original_code\"].strip()\n",
    "\n",
    "    return tokenizer(question, context, max_length=max_tokens, padding=padding)\n",
    "\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_sample)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_sample)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_sample)\n",
    "\n",
    "print(\n",
    "    \"Max number of tokens in tokenized train dataset: \",\n",
    "    len(max(tokenized_train_dataset[\"input_ids\"], key=len)),\n",
    ")\n",
    "print(\n",
    "    \"Max number of tokens in tokenized val dataset: \",\n",
    "    len(max(tokenized_val_dataset[\"input_ids\"], key=len)),\n",
    ")\n",
    "print(\n",
    "    \"Max number of tokens in tokenized test dataset: \",\n",
    "    len(max(tokenized_test_dataset[\"input_ids\"], key=len)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=<generator object filter_samples_below_number_of_tokens.<locals>.<genexpr> at 0x00000180500D7C30> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "max_length = 384\n",
    "\n",
    "\n",
    "def filter_samples_below_number_of_tokens(dataset, max_tokens: int):\n",
    "    indices_to_remove = []\n",
    "\n",
    "    # Find indices of samples where number of tokens exceeds max number of tokens\n",
    "    for index, sample in enumerate(dataset):\n",
    "        tokenized_sample = tokenize_sample(sample)\n",
    "        if len(tokenized_sample[\"input_ids\"]) > max_tokens:\n",
    "            indices_to_remove.append(index)\n",
    "\n",
    "    # Keep only samples with number of tokens less or equal than max number of tokens\n",
    "    dataset_indices = range(len(dataset))\n",
    "    filtered_dataset = dataset.select(\n",
    "        index for index in dataset_indices if index not in set(indices_to_remove)\n",
    "    )\n",
    "\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "filtered_train_dataset = filter_samples_below_number_of_tokens(\n",
    "    train_dataset, max_tokens=max_length\n",
    ")\n",
    "filtered_val_dataset = filter_samples_below_number_of_tokens(\n",
    "    val_dataset, max_tokens=max_length\n",
    ")\n",
    "filtered_test_dataset = filter_samples_below_number_of_tokens(\n",
    "    test_dataset, max_tokens=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in tokenized train dataset before filtering:  56080\n",
      "Number of samples in tokenized val dataset before filtering:  7000\n",
      "Number of samples in tokenized test dataset before filtering:  7000\n",
      "\n",
      "---------------\n",
      "\n",
      "Number of samples in tokenized train dataset after filtering:  55893\n",
      "Number of samples in tokenized val dataset after filtering:  6975\n",
      "Number of samples in tokenized test dataset after filtering:  6982\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Number of samples in tokenized train dataset before filtering: \",\n",
    "    len(train_dataset),\n",
    ")\n",
    "print(\"Number of samples in tokenized val dataset before filtering: \", len(val_dataset))\n",
    "print(\n",
    "    \"Number of samples in tokenized test dataset before filtering: \", len(test_dataset)\n",
    ")\n",
    "\n",
    "print(\"\\n---------------\\n\")\n",
    "\n",
    "print(\n",
    "    \"Number of samples in tokenized train dataset after filtering: \",\n",
    "    len(filtered_train_dataset),\n",
    ")\n",
    "print(\n",
    "    \"Number of samples in tokenized val dataset after filtering: \",\n",
    "    len(filtered_val_dataset),\n",
    ")\n",
    "print(\n",
    "    \"Number of samples in tokenized test dataset after filtering: \",\n",
    "    len(filtered_test_dataset),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    questions = [q.strip() for q in dataset[\"questions\"]]\n",
    "    contexts = [c.strip() for c in dataset[\"original_code\"]]\n",
    "    answers = [c.strip() for c in dataset[\"answers\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        text_target=answers,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d12e56abb344928c02d8b8be605d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/55893 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94b3abb839534055a1822e51833a1d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6975 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567f0fd0c7714955b25f99835cfec020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6982 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = filtered_train_dataset.map(\n",
    "    preprocess_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=filtered_train_dataset.column_names,\n",
    ")\n",
    "tokenized_val_dataset = filtered_val_dataset.map(\n",
    "    preprocess_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=filtered_val_dataset.column_names,\n",
    ")\n",
    "tokenized_test_dataset = filtered_test_dataset.map(\n",
    "    preprocess_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=filtered_test_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokenized train dataset entries have 384 tokens:  True\n",
      "All tokenized val dataset entries have 384 tokens:  True\n",
      "All tokenized test dataset entries have 384 tokens:  True\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"All tokenized train dataset entries have {max_length} tokens: \",\n",
    "    all(\n",
    "        [\n",
    "            len(input_ids) == max_length\n",
    "            for input_ids in tokenized_train_dataset[\"input_ids\"]\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    f\"All tokenized val dataset entries have {max_length} tokens: \",\n",
    "    all(\n",
    "        [\n",
    "            len(input_ids) == max_length\n",
    "            for input_ids in tokenized_val_dataset[\"input_ids\"]\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    f\"All tokenized test dataset entries have {max_length} tokens: \",\n",
    "    all(\n",
    "        [\n",
    "            len(input_ids) == max_length\n",
    "            for input_ids in tokenized_test_dataset[\"input_ids\"]\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "training_number = 1\n",
    "\n",
    "model_name = \"python-bert-uncased\"\n",
    "full_model_name = f\"{model_name}-{training_number}\"\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint_filename_template = constants.checkpoint_filename_template\n",
    "checkpoints_path = (\n",
    "    generative_qa_paths.training_checkpoints_dir\n",
    "    / full_model_name\n",
    "    / checkpoint_filename_template\n",
    ")\n",
    "\n",
    "# Hub\n",
    "hub_path = generative_qa_paths.hub_models_location / full_model_name\n",
    "\n",
    "# Saved models\n",
    "saved_models_path = generative_qa_paths.saved_models_dir / full_model_name\n",
    "\n",
    "# Figures\n",
    "figures_dir = generative_qa_paths.figures_dir / full_model_name\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 2\n",
    "train_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for fine-tuning\n",
    "# model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(model_checkpoint, model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"generative-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for fine-tuning\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"nlp-polish/generative-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for fine-tuning\n",
    "# model2 = TFAutoModelForSeq2SeqLM.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels'],\n",
       "    num_rows: 55893\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'labels'],\n",
       "    num_rows: 55893\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Make sure to set the decoder_start_token_id attribute of the model's configuration.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\STUDIA\\IPS\\question-answering\\generative-qa\\notebooks\\python\\python_bert-uncased_1_test.ipynb Cell 24\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Dataset preparation\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data_collator \u001b[39m=\u001b[39m DataCollatorForSeq2Seq(tokenizer, model\u001b[39m=\u001b[39mmodel, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m tf_train_dataset \u001b[39m=\u001b[39m core_qa_utils\u001b[39m.\u001b[39;49mprepare_tf_dataset(\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     hf_dataset\u001b[39m=\u001b[39;49mtokenized_train_dataset,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     collator\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m tf_val_dataset \u001b[39m=\u001b[39m core_qa_utils\u001b[39m.\u001b[39mprepare_tf_dataset(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     hf_dataset\u001b[39m=\u001b[39mtokenized_val_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m tf_test_dataset \u001b[39m=\u001b[39m core_qa_utils\u001b[39m.\u001b[39mprepare_tf_dataset(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     hf_dataset\u001b[39m=\u001b[39mtokenized_test_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/generative-qa/notebooks/python/python_bert-uncased_1_test.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n",
      "File \u001b[1;32mE:\\STUDIA\\IPS\\question-answering\\src\\question_answering\\utils\\core_qa_utils.py:99\u001b[0m, in \u001b[0;36mprepare_tf_dataset\u001b[1;34m(model, hf_dataset, collator, batch_size, shuffle)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_tf_dataset\u001b[39m(\n\u001b[0;32m     93\u001b[0m     model,\n\u001b[0;32m     94\u001b[0m     hf_dataset: Dataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m     shuffle: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     98\u001b[0m ):\n\u001b[1;32m---> 99\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mprepare_tf_dataset(\n\u001b[0;32m    100\u001b[0m         dataset\u001b[39m=\u001b[39;49mhf_dataset,\n\u001b[0;32m    101\u001b[0m         collate_fn\u001b[39m=\u001b[39;49mcollator,\n\u001b[0;32m    102\u001b[0m         shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m    103\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m    104\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1456\u001b[0m, in \u001b[0;36mTFPreTrainedModel.prepare_tf_dataset\u001b[1;34m(self, dataset, batch_size, shuffle, tokenizer, collate_fn, collate_fn_args, drop_remainder, prefetch)\u001b[0m\n\u001b[0;32m   1454\u001b[0m model_labels \u001b[39m=\u001b[39m find_labels(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m   1455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcols_to_retain\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(inspect\u001b[39m.\u001b[39msignature(dataset\u001b[39m.\u001b[39m_get_output_signature)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mkeys()):\n\u001b[1;32m-> 1456\u001b[0m     output_signature, _ \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49m_get_output_signature(\n\u001b[0;32m   1457\u001b[0m         dataset,\n\u001b[0;32m   1458\u001b[0m         batch_size\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1459\u001b[0m         collate_fn\u001b[39m=\u001b[39;49mcollate_fn,\n\u001b[0;32m   1460\u001b[0m         collate_fn_args\u001b[39m=\u001b[39;49mcollate_fn_args,\n\u001b[0;32m   1461\u001b[0m         cols_to_retain\u001b[39m=\u001b[39;49mmodel_inputs,\n\u001b[0;32m   1462\u001b[0m     )\n\u001b[0;32m   1463\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1464\u001b[0m     \u001b[39m# TODO Matt: This is a workaround for older versions of datasets that are missing the `cols_to_retain`\u001b[39;00m\n\u001b[0;32m   1465\u001b[0m     \u001b[39m#            argument. We should remove this once the minimum supported version of datasets is > 2.3.2\u001b[39;00m\n\u001b[0;32m   1466\u001b[0m     unwanted_columns \u001b[39m=\u001b[39m [\n\u001b[0;32m   1467\u001b[0m         feature\n\u001b[0;32m   1468\u001b[0m         \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mfeatures\n\u001b[0;32m   1469\u001b[0m         \u001b[39mif\u001b[39;00m feature \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_inputs \u001b[39mand\u001b[39;00m feature \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mlabel_ids\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1470\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\datasets\\arrow_dataset.py:279\u001b[0m, in \u001b[0;36mTensorflowDatasetMixin._get_output_signature\u001b[1;34m(dataset, collate_fn, collate_fn_args, cols_to_retain, batch_size, num_test_batches)\u001b[0m\n\u001b[0;32m    277\u001b[0m         test_batch \u001b[39m=\u001b[39m {key: value \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m test_batch\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m cols_to_retain}\n\u001b[0;32m    278\u001b[0m     test_batch \u001b[39m=\u001b[39m [{key: value[i] \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m test_batch\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(test_batch_size)]\n\u001b[1;32m--> 279\u001b[0m     test_batch \u001b[39m=\u001b[39m collate_fn(test_batch, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcollate_fn_args)\n\u001b[0;32m    280\u001b[0m     test_batches\u001b[39m.\u001b[39mappend(test_batch)\n\u001b[0;32m    282\u001b[0m tf_columns_to_signatures \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\transformers\\data\\data_collator.py:600\u001b[0m, in \u001b[0;36mDataCollatorForSeq2Seq.__call__\u001b[1;34m(self, features, return_tensors)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39m# prepare decoder_input_ids\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    596\u001b[0m     labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    597\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    598\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39m\"\u001b[39m\u001b[39mprepare_decoder_input_ids_from_labels\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    599\u001b[0m ):\n\u001b[1;32m--> 600\u001b[0m     decoder_input_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mprepare_decoder_input_ids_from_labels(labels\u001b[39m=\u001b[39;49mfeatures[\u001b[39m\"\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    601\u001b[0m     features[\u001b[39m\"\u001b[39m\u001b[39mdecoder_input_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m decoder_input_ids\n\u001b[0;32m    603\u001b[0m \u001b[39mreturn\u001b[39;00m features\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_tf_encoder_decoder.py:656\u001b[0m, in \u001b[0;36mTFEncoderDecoderModel.prepare_decoder_input_ids_from_labels\u001b[1;34m(self, labels)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_decoder_input_ids_from_labels\u001b[39m(\u001b[39mself\u001b[39m, labels: tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 656\u001b[0m     \u001b[39mreturn\u001b[39;00m shift_tokens_right(labels, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mpad_token_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdecoder_start_token_id)\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\question_answering\\lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_tf_encoder_decoder.py:172\u001b[0m, in \u001b[0;36mshift_tokens_right\u001b[1;34m(input_ids, pad_token_id, decoder_start_token_id)\u001b[0m\n\u001b[0;32m    169\u001b[0m pad_token_id \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(pad_token_id, input_ids\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m decoder_start_token_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMake sure to set the decoder_start_token_id attribute of the model\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms configuration.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    173\u001b[0m decoder_start_token_id \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(decoder_start_token_id, input_ids\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    175\u001b[0m start_tokens \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfill((shape_list(input_ids)[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m), decoder_start_token_id)\n",
      "\u001b[1;31mValueError\u001b[0m: Make sure to set the decoder_start_token_id attribute of the model's configuration."
     ]
    }
   ],
   "source": [
    "# Dataset preparation\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")\n",
    "\n",
    "tf_train_dataset = core_qa_utils.prepare_tf_dataset(\n",
    "    model=model,\n",
    "    hf_dataset=tokenized_train_dataset,\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "tf_val_dataset = core_qa_utils.prepare_tf_dataset(\n",
    "    model=model,\n",
    "    hf_dataset=tokenized_val_dataset,\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "tf_test_dataset = core_qa_utils.prepare_tf_dataset(\n",
    "    model=model,\n",
    "    hf_dataset=tokenized_test_dataset,\n",
    "    collator=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoints_path, verbose=1, save_weights_only=True\n",
    ")\n",
    "early_stop_cb = tf.keras.callbacks.EarlyStopping(patience=1)\n",
    "# push_to_hub = keras_callbacks.PushToHubCallback(\n",
    "#     output_dir=full_model_name, tokenizer=tokenizer\n",
    "# )\n",
    "\n",
    "callbacks = [\n",
    "    checkpoint_cb,\n",
    "    early_stop_cb,\n",
    "    # push_to_hub\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "num_train_steps = len(tf_train_dataset) * train_epochs\n",
    "\n",
    "lr_scheduler = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# Compile\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [\"accuracy\"]\n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "model.compile(optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the new data\n",
    "history = model.fit(\n",
    "    tf_train_dataset,\n",
    "    validation_data=tf_val_dataset,\n",
    "    epochs=train_epochs,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "question_answering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
