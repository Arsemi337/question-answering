METRICSY QA generative:
- train, val, test losses & wykresy
- procent odpowiedzi zamkniętych do pytań zamkniętych i odwrotnie
- czas szkolenia a skuteczność
- bleu, meteor, rogue
- manualne wybiórcze sprawdzanie (merytoryka - sprawdzanie czy wygenerowane zdania mówią to co te referencyjne, gramatyka - samodzielne sprawdzanie vs jakiś model do tego sprawdzający gramatykę albo na około z modelem poprawiającym jeśli by się dało)
- jakieś propozycje?

METRICSY QA extractive:
- exact match (wykresy?)
- exact match z tolerancją dla indeksu początkowego i końcowego (artur opisze)
- f1 score, precision, recall (conf matrix)
- czas szkolenia a skuteczność
- train, val, test losses & wykresy
+ dodatkowe wykresy/analizy na podstawie tego co zauważymy w trakcie (np. exact matche w zależności od długości pytań)
+ jakieś propozycje?

Oznaczenia modeli:
bs - batch size

Przy każdym trenowanym modelu ekstraktywnym powinny być zapisane:
wykresy lossów, wykresy accuracy, precision, recall, f1, exact_match, czas szkolenia, i które GPU. Albo trzymanie ich właśnie w notebookach ewaluacyjnych?

Wydzielanie plików treningowych i ewaluacyjnych

Każde szkolenie to nowy notebook? Jeśli tak to jakie nazewnictwo?
