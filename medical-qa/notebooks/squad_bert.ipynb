{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T18:41:11.118213700Z",
     "start_time": "2023-10-22T18:41:11.071877500Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering, DefaultDataCollator, create_optimizer\n",
    "import datasets\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b7529904f21909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T17:57:37.227199700Z",
     "start_time": "2023-10-22T17:57:36.275971600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\pyarrow\\pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "def load_train_val_test_datasets(dataset_path='./../data/datasets/squad'):\n",
    "    train = pd.read_csv(f'{dataset_path}/train.csv').dropna()\n",
    "    val = pd.read_csv(f'{dataset_path}/dev.csv').dropna()\n",
    "    test = pd.read_csv(f'{dataset_path}/test.csv').dropna()\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def convert_dataframes_to_datasets(dataframes: list):\n",
    "    return tuple(\n",
    "        [datasets.Dataset.from_pandas(dataframe, preserve_index=False) for dataframe in\n",
    "         dataframes])\n",
    "\n",
    "\n",
    "df_train, df_val, df_test = load_train_val_test_datasets()\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = convert_dataframes_to_datasets([df_train, df_val, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cd82f21a8585dde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T17:57:37.244478800Z",
     "start_time": "2023-10-22T17:57:37.228199Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['index', 'id', 'context', 'question', 'answer_text', 'answer_start'],\n",
       "     num_rows: 68716\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['index', 'id', 'context', 'question', 'answer_text', 'answer_start'],\n",
       "     num_rows: 14724\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['index', 'id', 'context', 'question', 'answer_text', 'answer_start'],\n",
       "     num_rows: 14725\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db291e0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T17:57:37.632104700Z",
     "start_time": "2023-10-22T17:57:37.244478800Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d177b07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T17:58:31.702548100Z",
     "start_time": "2023-10-22T17:57:37.635106100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcbd25f450442a6a218817a8184b7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/68716 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b841b8471f9b4672bddf5be1fdc57a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a58f16b9b043cfbb35bdf62bcac711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14725 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of tokens in tokenized train dataset:  882\n",
      "Max number of tokens in tokenized val dataset:  879\n",
      "Max number of tokens in tokenized test dataset:  831\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sample(sample, max_tokens=None, padding=False):\n",
    "    question = sample['question'].strip()\n",
    "    context = sample['context'].strip()\n",
    "\n",
    "    return tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        max_length=max_tokens,\n",
    "        padding=padding\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_sample)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_sample)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_sample)\n",
    "\n",
    "print('Max number of tokens in tokenized train dataset: ', len(max(tokenized_train_dataset['input_ids'], key=len)))\n",
    "print('Max number of tokens in tokenized val dataset: ', len(max(tokenized_val_dataset['input_ids'], key=len)))\n",
    "print('Max number of tokens in tokenized test dataset: ', len(max(tokenized_test_dataset['input_ids'], key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e53325b1b3a28b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T17:59:11.418791900Z",
     "start_time": "2023-10-22T17:58:31.676515800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=<generator object filter_samples_below_number_of_tokens.<locals>.<genexpr> at 0x00000211EB274CF0> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "max_length = 384\n",
    "\n",
    "\n",
    "def filter_samples_below_number_of_tokens(dataset, max_tokens: int):\n",
    "    indices_to_remove = []\n",
    "\n",
    "    # Find indices of samples where number of tokens exceeds max number of tokens\n",
    "    for index, sample in enumerate(dataset):\n",
    "        tokenized_sample = tokenize_sample(sample)\n",
    "        if len(tokenized_sample['input_ids']) > max_tokens:\n",
    "            indices_to_remove.append(index)\n",
    "\n",
    "    # Keep only samples with number of tokens less or equal than max number of tokens\n",
    "    dataset_indices = range(len(dataset))\n",
    "    filtered_dataset = dataset.select(\n",
    "        index for index in dataset_indices if index not in set(indices_to_remove)\n",
    "    )\n",
    "\n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "filtered_train_dataset = filter_samples_below_number_of_tokens(train_dataset, max_tokens=max_length)\n",
    "filtered_val_dataset = filter_samples_below_number_of_tokens(val_dataset, max_tokens=max_length)\n",
    "filtered_test_dataset = filter_samples_below_number_of_tokens(test_dataset, max_tokens=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9373fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Along with the United Democratic Party, what party currently rules the Marshall Islands?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_train_dataset['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "755eac50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "633"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_char = filtered_train_dataset['answer_start'][0]\n",
    "start_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae9d1d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the AKA'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_text = filtered_train_dataset['answer_text'][0]\n",
    "answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16c44f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_char = start_char + len(answer_text)\n",
    "end_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4215b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'56f961049b226e1400dd13eb'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_train_dataset['id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1aebcb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T17:59:11.434129300Z",
     "start_time": "2023-10-22T17:59:11.419793800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in tokenized train dataset before filtering:  68716\n",
      "Number of samples in tokenized val dataset before filtering:  14724\n",
      "Number of samples in tokenized test dataset before filtering:  14725\n",
      "\n",
      "---------------\n",
      "\n",
      "Number of samples in tokenized train dataset after filtering:  67801\n",
      "Number of samples in tokenized val dataset after filtering:  14541\n",
      "Number of samples in tokenized test dataset after filtering:  14519\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples in tokenized train dataset before filtering: ', len(train_dataset))\n",
    "print('Number of samples in tokenized val dataset before filtering: ', len(val_dataset))\n",
    "print('Number of samples in tokenized test dataset before filtering: ', len(test_dataset))\n",
    "\n",
    "print('\\n---------------\\n')\n",
    "\n",
    "print('Number of samples in tokenized train dataset after filtering: ', len(filtered_train_dataset))\n",
    "print('Number of samples in tokenized val dataset after filtering: ', len(filtered_val_dataset))\n",
    "print('Number of samples in tokenized test dataset after filtering: ', len(filtered_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7d867d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'id', 'context', 'question', 'answer_text', 'answer_start'],\n",
       "    num_rows: 67801\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a43a177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642c92755ffa4756bf0fef630afb4e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67801 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'id', 'context', 'question', 'answer_text', 'answer_start', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 67801\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(examples):\n",
    "    questions = [q.strip() for q in examples['question']]\n",
    "    # contexts = [c.strip() for c in examples['context']]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples['context'],\n",
    "        max_length=max_length,\n",
    "        padding='max_length'\n",
    "    )\n",
    "\n",
    "    return inputs\n",
    "\n",
    "b = filtered_train_dataset.map(tokenize, batched=True)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb6cdd06c7a27f8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:08:00.696127200Z",
     "start_time": "2023-10-22T19:07:02.299178600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def tokenize(examples):\n",
    "#     questions = [q.strip() for q in examples['question']]\n",
    "#     # contexts = [c.strip() for c in examples['context']]\n",
    "\n",
    "#     inputs = tokenizer(\n",
    "#         questions,\n",
    "#         examples['context'],\n",
    "#         max_length=max_length,\n",
    "#         padding='max_length'\n",
    "#     )\n",
    "\n",
    "#     return inputs\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    questions = [q.strip() for q in dataset['question']]\n",
    "    # contexts = [c.strip() for c in examples['context']]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        dataset['context'],\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        return_overflowing_tokens=True,\n",
    "    )\n",
    "    # sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    # for key, values in dataset.items():\n",
    "    #     inputs[key] = [values[i] for i in sample_map]\n",
    "    # dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "    answer_start_indices = dataset['answer_start']\n",
    "    answer_texts = dataset['answer_text']\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for index, _ in enumerate(inputs):\n",
    "        start_char = answer_start_indices[index]\n",
    "        end_char = start_char + len(answer_texts[index])\n",
    "\n",
    "        start_positions.append(start_char)\n",
    "        end_positions.append(end_char)\n",
    "\n",
    "    # inputs = inputs.add_column('start_positions', start_positions)\n",
    "    # inputs = inputs.add_column('end_positions', end_positions)\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "\n",
    "tokenized_train_dataset = preprocess_dataset(filtered_train_dataset)\n",
    "tokenized_val_dataset = preprocess_dataset(filtered_val_dataset)\n",
    "tokenized_test_dataset = preprocess_dataset(filtered_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a18a924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'id', 'context', 'question', 'answer_text', 'answer_start'],\n",
       "    num_rows: 67801\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b72cf37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd63eb3d71a4b2da3432f5c816a9bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67801 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 4 named start_positions expected length 1000 but got length 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\STUDIA\\IPS\\question-answering\\medical-qa\\notebooks\\squad_bert.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokenized_train_dataset \u001b[39m=\u001b[39m filtered_train_dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     preprocess_dataset,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#X54sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     remove_columns\u001b[39m=\u001b[39;49mfiltered_train_dataset\u001b[39m.\u001b[39;49mcolumn_names,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mlen\u001b[39m(filtered_train_dataset), \u001b[39mlen\u001b[39m(train_dataset)\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\datasets\\arrow_dataset.py:578\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    577\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 578\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    579\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    580\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    581\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\datasets\\arrow_dataset.py:543\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    537\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    538\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    539\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    540\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    541\u001b[0m }\n\u001b[0;32m    542\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 543\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    544\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    545\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\datasets\\arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3065\u001b[0m \u001b[39mif\u001b[39;00m transformed_dataset \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3066\u001b[0m     \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[0;32m   3067\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3068\u001b[0m         unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3071\u001b[0m         desc\u001b[39m=\u001b[39mdesc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   3072\u001b[0m     ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3073\u001b[0m         \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m Dataset\u001b[39m.\u001b[39m_map_single(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3074\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m   3075\u001b[0m                 shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\datasets\\arrow_dataset.py:3466\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3464\u001b[0m         writer\u001b[39m.\u001b[39mwrite_table(batch)\n\u001b[0;32m   3465\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 3466\u001b[0m         writer\u001b[39m.\u001b[39;49mwrite_batch(batch)\n\u001b[0;32m   3467\u001b[0m num_examples_progress_update \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m num_examples_in_batch\n\u001b[0;32m   3468\u001b[0m \u001b[39mif\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m _time \u001b[39m+\u001b[39m config\u001b[39m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\datasets\\arrow_writer.py:554\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    552\u001b[0m         inferred_features[col] \u001b[39m=\u001b[39m typed_sequence\u001b[39m.\u001b[39mget_inferred_type()\n\u001b[0;32m    553\u001b[0m schema \u001b[39m=\u001b[39m inferred_features\u001b[39m.\u001b[39marrow_schema \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpa_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mschema\n\u001b[1;32m--> 554\u001b[0m pa_table \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_arrays(arrays, schema\u001b[39m=\u001b[39;49mschema)\n\u001b[0;32m    555\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\pyarrow\\table.pxi:3674\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\pyarrow\\table.pxi:2837\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\pyarrow\\error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Column 4 named start_positions expected length 1000 but got length 4"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = filtered_train_dataset.map(\n",
    "    preprocess_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=filtered_train_dataset.column_names,\n",
    ")\n",
    "len(filtered_train_dataset), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ff10a8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc093e38a8b9eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:08:11.588608600Z",
     "start_time": "2023-10-22T19:08:00.713638Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokenized train dataset entries have 384 tokens:  True\n",
      "All tokenized val dataset entries have 384 tokens:  True\n",
      "All tokenized test dataset entries have 384 tokens:  True\n"
     ]
    }
   ],
   "source": [
    "print(f'All tokenized train dataset entries have {max_length} tokens: ',\n",
    "      all([len(input_ids) == max_length for input_ids in tokenized_train_dataset['input_ids']]))\n",
    "print(f'All tokenized val dataset entries have {max_length} tokens: ',\n",
    "      all([len(input_ids) == max_length for input_ids in tokenized_val_dataset['input_ids']]))\n",
    "print(f'All tokenized test dataset entries have {max_length} tokens: ',\n",
    "      all([len(input_ids) == max_length for input_ids in tokenized_test_dataset['input_ids']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9d5bedb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:11:09.188838900Z",
     "start_time": "2023-10-22T19:11:08.245208300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6038e6a163d17a8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:11:09.201367800Z",
     "start_time": "2023-10-22T19:11:09.187333100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator(return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67fd75ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[633,\n",
       " 494,\n",
       " 22,\n",
       " 53,\n",
       " 585,\n",
       " 287,\n",
       " 121,\n",
       " 391,\n",
       " 565,\n",
       " 141,\n",
       " 396,\n",
       " 629,\n",
       " 49,\n",
       " 7,\n",
       " 453,\n",
       " 108,\n",
       " 35,\n",
       " 560,\n",
       " 130,\n",
       " 181,\n",
       " 178,\n",
       " 26,\n",
       " 393,\n",
       " 749,\n",
       " 413,\n",
       " 35,\n",
       " 167,\n",
       " 0,\n",
       " 63,\n",
       " 29,\n",
       " 808,\n",
       " 486,\n",
       " 89,\n",
       " 92,\n",
       " 0,\n",
       " 173,\n",
       " 342,\n",
       " 821,\n",
       " 170,\n",
       " 83,\n",
       " 367,\n",
       " 441,\n",
       " 533,\n",
       " 427,\n",
       " 197,\n",
       " 124,\n",
       " 42,\n",
       " 168,\n",
       " 192,\n",
       " 455,\n",
       " 372,\n",
       " 20,\n",
       " 404,\n",
       " 141,\n",
       " 979,\n",
       " 410,\n",
       " 87,\n",
       " 579,\n",
       " 1076,\n",
       " 533,\n",
       " 386,\n",
       " 348,\n",
       " 173,\n",
       " 550,\n",
       " 191,\n",
       " 287,\n",
       " 714,\n",
       " 256,\n",
       " 534,\n",
       " 305,\n",
       " 0,\n",
       " 207,\n",
       " 269,\n",
       " 327,\n",
       " 1287,\n",
       " 463,\n",
       " 36,\n",
       " 129,\n",
       " 302,\n",
       " 41,\n",
       " 19,\n",
       " 619,\n",
       " 1144,\n",
       " 325,\n",
       " 599,\n",
       " 150,\n",
       " 447,\n",
       " 77,\n",
       " 498,\n",
       " 92,\n",
       " 243,\n",
       " 62,\n",
       " 693,\n",
       " 119,\n",
       " 0,\n",
       " 465,\n",
       " 473,\n",
       " 258,\n",
       " 397,\n",
       " 428,\n",
       " 172,\n",
       " 52,\n",
       " 539,\n",
       " 602,\n",
       " 0,\n",
       " 474,\n",
       " 477,\n",
       " 183,\n",
       " 485,\n",
       " 364,\n",
       " 82,\n",
       " 104,\n",
       " 116,\n",
       " 129,\n",
       " 373,\n",
       " 55,\n",
       " 199,\n",
       " 125,\n",
       " 397,\n",
       " 293,\n",
       " 270,\n",
       " 19,\n",
       " 51,\n",
       " 509,\n",
       " 113,\n",
       " 311,\n",
       " 81,\n",
       " 546,\n",
       " 736,\n",
       " 717,\n",
       " 324,\n",
       " 43,\n",
       " 425,\n",
       " 303,\n",
       " 1222,\n",
       " 782,\n",
       " 685,\n",
       " 377,\n",
       " 67,\n",
       " 712,\n",
       " 86,\n",
       " 36,\n",
       " 163,\n",
       " 44,\n",
       " 10,\n",
       " 178,\n",
       " 242,\n",
       " 729,\n",
       " 870,\n",
       " 229,\n",
       " 97,\n",
       " 235,\n",
       " 921,\n",
       " 842,\n",
       " 114,\n",
       " 158,\n",
       " 7,\n",
       " 475,\n",
       " 605,\n",
       " 226,\n",
       " 776,\n",
       " 310,\n",
       " 417,\n",
       " 416,\n",
       " 110,\n",
       " 117,\n",
       " 342,\n",
       " 583,\n",
       " 116,\n",
       " 535,\n",
       " 127,\n",
       " 484,\n",
       " 249,\n",
       " 84,\n",
       " 478,\n",
       " 83,\n",
       " 545,\n",
       " 1751,\n",
       " 86,\n",
       " 7,\n",
       " 1353,\n",
       " 210,\n",
       " 521,\n",
       " 0,\n",
       " 209,\n",
       " 785,\n",
       " 366,\n",
       " 276,\n",
       " 211,\n",
       " 25,\n",
       " 26,\n",
       " 86,\n",
       " 796,\n",
       " 177,\n",
       " 49,\n",
       " 94,\n",
       " 677,\n",
       " 81,\n",
       " 702,\n",
       " 355,\n",
       " 183,\n",
       " 123,\n",
       " 130,\n",
       " 253,\n",
       " 254,\n",
       " 795,\n",
       " 261,\n",
       " 316,\n",
       " 3,\n",
       " 11,\n",
       " 509,\n",
       " 330,\n",
       " 138,\n",
       " 125,\n",
       " 430,\n",
       " 38,\n",
       " 46,\n",
       " 207,\n",
       " 824,\n",
       " 22,\n",
       " 942,\n",
       " 640,\n",
       " 259,\n",
       " 302,\n",
       " 133,\n",
       " 73,\n",
       " 311,\n",
       " 902,\n",
       " 364,\n",
       " 302,\n",
       " 86,\n",
       " 87,\n",
       " 194,\n",
       " 428,\n",
       " 214,\n",
       " 229,\n",
       " 372,\n",
       " 251,\n",
       " 391,\n",
       " 220,\n",
       " 189,\n",
       " 102,\n",
       " 640,\n",
       " 435,\n",
       " 107,\n",
       " 422,\n",
       " 81,\n",
       " 922,\n",
       " 176,\n",
       " 48,\n",
       " 48,\n",
       " 795,\n",
       " 45,\n",
       " 211,\n",
       " 20,\n",
       " 553,\n",
       " 880,\n",
       " 484,\n",
       " 154,\n",
       " 453,\n",
       " 0,\n",
       " 463,\n",
       " 375,\n",
       " 273,\n",
       " 633,\n",
       " 64,\n",
       " 259,\n",
       " 284,\n",
       " 133,\n",
       " 359,\n",
       " 679,\n",
       " 508,\n",
       " 503,\n",
       " 245,\n",
       " 37,\n",
       " 11,\n",
       " 458,\n",
       " 196,\n",
       " 797,\n",
       " 276,\n",
       " 180,\n",
       " 97,\n",
       " 4,\n",
       " 142,\n",
       " 45,\n",
       " 221,\n",
       " 262,\n",
       " 58,\n",
       " 5,\n",
       " 291,\n",
       " 212,\n",
       " 314,\n",
       " 142,\n",
       " 202,\n",
       " 1299,\n",
       " 3,\n",
       " 250,\n",
       " 588,\n",
       " 185,\n",
       " 200,\n",
       " 725,\n",
       " 9,\n",
       " 413,\n",
       " 1131,\n",
       " 901,\n",
       " 153,\n",
       " 424,\n",
       " 295,\n",
       " 237,\n",
       " 71,\n",
       " 369,\n",
       " 375,\n",
       " 1602,\n",
       " 0,\n",
       " 642,\n",
       " 188,\n",
       " 858,\n",
       " 480,\n",
       " 109,\n",
       " 318,\n",
       " 523,\n",
       " 868,\n",
       " 532,\n",
       " 472,\n",
       " 638,\n",
       " 855,\n",
       " 18,\n",
       " 190,\n",
       " 266,\n",
       " 941,\n",
       " 378,\n",
       " 637,\n",
       " 192,\n",
       " 269,\n",
       " 163,\n",
       " 392,\n",
       " 94,\n",
       " 139,\n",
       " 0,\n",
       " 547,\n",
       " 120,\n",
       " 122,\n",
       " 535,\n",
       " 3,\n",
       " 0,\n",
       " 782,\n",
       " 354,\n",
       " 94,\n",
       " 812,\n",
       " 35,\n",
       " 34,\n",
       " 179,\n",
       " 170,\n",
       " 208,\n",
       " 7,\n",
       " 147,\n",
       " 0,\n",
       " 680,\n",
       " 150,\n",
       " 0,\n",
       " 342,\n",
       " 864,\n",
       " 363,\n",
       " 552,\n",
       " 1220,\n",
       " 120,\n",
       " 637,\n",
       " 431,\n",
       " 260,\n",
       " 13,\n",
       " 74,\n",
       " 45,\n",
       " 67,\n",
       " 438,\n",
       " 998,\n",
       " 152,\n",
       " 506,\n",
       " 926,\n",
       " 893,\n",
       " 362,\n",
       " 83,\n",
       " 132,\n",
       " 254,\n",
       " 548,\n",
       " 28,\n",
       " 188,\n",
       " 138,\n",
       " 337,\n",
       " 261,\n",
       " 1070,\n",
       " 60,\n",
       " 133,\n",
       " 755,\n",
       " 16,\n",
       " 9,\n",
       " 61,\n",
       " 43,\n",
       " 345,\n",
       " 148,\n",
       " 320,\n",
       " 0,\n",
       " 104,\n",
       " 606,\n",
       " 390,\n",
       " 112,\n",
       " 736,\n",
       " 164,\n",
       " 0,\n",
       " 542,\n",
       " 481,\n",
       " 62,\n",
       " 251,\n",
       " 326,\n",
       " 104,\n",
       " 699,\n",
       " 164,\n",
       " 298,\n",
       " 344,\n",
       " 384,\n",
       " 354,\n",
       " 61,\n",
       " 945,\n",
       " 360,\n",
       " 258,\n",
       " 115,\n",
       " 104,\n",
       " 522,\n",
       " 407,\n",
       " 189,\n",
       " 320,\n",
       " 181,\n",
       " 67,\n",
       " 70,\n",
       " 316,\n",
       " 147,\n",
       " 78,\n",
       " 390,\n",
       " 97,\n",
       " 589,\n",
       " 110,\n",
       " 195,\n",
       " 378,\n",
       " 315,\n",
       " 367,\n",
       " 62,\n",
       " 335,\n",
       " 793,\n",
       " 24,\n",
       " 141,\n",
       " 951,\n",
       " 65,\n",
       " 329,\n",
       " 151,\n",
       " 545,\n",
       " 111,\n",
       " 675,\n",
       " 14,\n",
       " 385,\n",
       " 807,\n",
       " 374,\n",
       " 556,\n",
       " 834,\n",
       " 251,\n",
       " 287,\n",
       " 642,\n",
       " 13,\n",
       " 97,\n",
       " 384,\n",
       " 26,\n",
       " 3,\n",
       " 668,\n",
       " 64,\n",
       " 336,\n",
       " 515,\n",
       " 393,\n",
       " 916,\n",
       " 223,\n",
       " 661,\n",
       " 257,\n",
       " 172,\n",
       " 526,\n",
       " 111,\n",
       " 252,\n",
       " 177,\n",
       " 881,\n",
       " 63,\n",
       " 358,\n",
       " 571,\n",
       " 381,\n",
       " 231,\n",
       " 29,\n",
       " 521,\n",
       " 350,\n",
       " 151,\n",
       " 565,\n",
       " 212,\n",
       " 927,\n",
       " 137,\n",
       " 790,\n",
       " 366,\n",
       " 591,\n",
       " 271,\n",
       " 744,\n",
       " 40,\n",
       " 584,\n",
       " 99,\n",
       " 199,\n",
       " 7,\n",
       " 512,\n",
       " 795,\n",
       " 121,\n",
       " 80,\n",
       " 579,\n",
       " 488,\n",
       " 272,\n",
       " 235,\n",
       " 585,\n",
       " 216,\n",
       " 372,\n",
       " 555,\n",
       " 378,\n",
       " 519,\n",
       " 1012,\n",
       " 50,\n",
       " 124,\n",
       " 0,\n",
       " 254,\n",
       " 172,\n",
       " 123,\n",
       " 258,\n",
       " 839,\n",
       " 109,\n",
       " 171,\n",
       " 610,\n",
       " 26,\n",
       " 129,\n",
       " 283,\n",
       " 47,\n",
       " 7,\n",
       " 151,\n",
       " 226,\n",
       " 624,\n",
       " 690,\n",
       " 236,\n",
       " 924,\n",
       " 239,\n",
       " 4,\n",
       " 185,\n",
       " 264,\n",
       " 563,\n",
       " 11,\n",
       " 279,\n",
       " 460,\n",
       " 727,\n",
       " 109,\n",
       " 48,\n",
       " 564,\n",
       " 751,\n",
       " 143,\n",
       " 61,\n",
       " 138,\n",
       " 155,\n",
       " 436,\n",
       " 498,\n",
       " 677,\n",
       " 319,\n",
       " 242,\n",
       " 531,\n",
       " 487,\n",
       " 652,\n",
       " 106,\n",
       " 95,\n",
       " 365,\n",
       " 525,\n",
       " 0,\n",
       " 63,\n",
       " 159,\n",
       " 0,\n",
       " 594,\n",
       " 749,\n",
       " 533,\n",
       " 134,\n",
       " 449,\n",
       " 370,\n",
       " 587,\n",
       " 20,\n",
       " 236,\n",
       " 27,\n",
       " 73,\n",
       " 509,\n",
       " 550,\n",
       " 761,\n",
       " 218,\n",
       " 863,\n",
       " 153,\n",
       " 699,\n",
       " 296,\n",
       " 76,\n",
       " 164,\n",
       " 591,\n",
       " 161,\n",
       " 145,\n",
       " 57,\n",
       " 314,\n",
       " 79,\n",
       " 474,\n",
       " 506,\n",
       " 260,\n",
       " 129,\n",
       " 152,\n",
       " 432,\n",
       " 60,\n",
       " 226,\n",
       " 135,\n",
       " 972,\n",
       " 6,\n",
       " 15,\n",
       " 168,\n",
       " 601,\n",
       " 309,\n",
       " 437,\n",
       " 757,\n",
       " 486,\n",
       " 487,\n",
       " 837,\n",
       " 235,\n",
       " 93,\n",
       " 188,\n",
       " 888,\n",
       " 22,\n",
       " 803,\n",
       " 102,\n",
       " 150,\n",
       " 262,\n",
       " 56,\n",
       " 236,\n",
       " 376,\n",
       " 224,\n",
       " 18,\n",
       " 90,\n",
       " 142,\n",
       " 41,\n",
       " 400,\n",
       " 262,\n",
       " 299,\n",
       " 454,\n",
       " 137,\n",
       " 732,\n",
       " 27,\n",
       " 61,\n",
       " 95,\n",
       " 109,\n",
       " 325,\n",
       " 151,\n",
       " 46,\n",
       " 93,\n",
       " 129,\n",
       " 155,\n",
       " 48,\n",
       " 891,\n",
       " 737,\n",
       " 13,\n",
       " 45,\n",
       " 361,\n",
       " 145,\n",
       " 585,\n",
       " 46,\n",
       " 85,\n",
       " 498,\n",
       " 227,\n",
       " 475,\n",
       " 294,\n",
       " 219,\n",
       " 274,\n",
       " 349,\n",
       " 324,\n",
       " 603,\n",
       " 198,\n",
       " 219,\n",
       " 203,\n",
       " 352,\n",
       " 897,\n",
       " 362,\n",
       " 92,\n",
       " 48,\n",
       " 69,\n",
       " 30,\n",
       " 499,\n",
       " 95,\n",
       " 26,\n",
       " 521,\n",
       " 0,\n",
       " 597,\n",
       " 16,\n",
       " 457,\n",
       " 287,\n",
       " 595,\n",
       " 174,\n",
       " 324,\n",
       " 504,\n",
       " 659,\n",
       " 301,\n",
       " 559,\n",
       " 605,\n",
       " 82,\n",
       " 41,\n",
       " 40,\n",
       " 581,\n",
       " 229,\n",
       " 213,\n",
       " 210,\n",
       " 466,\n",
       " 709,\n",
       " 185,\n",
       " 276,\n",
       " 94,\n",
       " 361,\n",
       " 61,\n",
       " 286,\n",
       " 231,\n",
       " 562,\n",
       " 149,\n",
       " 144,\n",
       " 354,\n",
       " 319,\n",
       " 0,\n",
       " 0,\n",
       " 397,\n",
       " 51,\n",
       " 91,\n",
       " 300,\n",
       " 263,\n",
       " 496,\n",
       " 149,\n",
       " 252,\n",
       " 255,\n",
       " 190,\n",
       " 245,\n",
       " 223,\n",
       " 209,\n",
       " 719,\n",
       " 438,\n",
       " 255,\n",
       " 360,\n",
       " 367,\n",
       " 170,\n",
       " 530,\n",
       " 315,\n",
       " 158,\n",
       " 356,\n",
       " 161,\n",
       " 739,\n",
       " 86,\n",
       " 228,\n",
       " 97,\n",
       " 38,\n",
       " 595,\n",
       " 176,\n",
       " 494,\n",
       " 304,\n",
       " 48,\n",
       " 143,\n",
       " 195,\n",
       " 245,\n",
       " 434,\n",
       " 200,\n",
       " 236,\n",
       " 129,\n",
       " 171,\n",
       " 304,\n",
       " 215,\n",
       " 292,\n",
       " 456,\n",
       " 211,\n",
       " 402,\n",
       " 165,\n",
       " 1059,\n",
       " 190,\n",
       " 141,\n",
       " 643,\n",
       " 218,\n",
       " 501,\n",
       " 327,\n",
       " 1185,\n",
       " 490,\n",
       " 210,\n",
       " 319,\n",
       " 652,\n",
       " 113,\n",
       " 562,\n",
       " 221,\n",
       " 690,\n",
       " 0,\n",
       " 72,\n",
       " 387,\n",
       " 168,\n",
       " 214,\n",
       " 268,\n",
       " 263,\n",
       " 243,\n",
       " 315,\n",
       " 25,\n",
       " 339,\n",
       " 138,\n",
       " 3,\n",
       " 25,\n",
       " 356,\n",
       " 1214,\n",
       " 448,\n",
       " 15,\n",
       " 164,\n",
       " 530,\n",
       " 778,\n",
       " 373,\n",
       " 420,\n",
       " 108,\n",
       " 504,\n",
       " 433,\n",
       " 371,\n",
       " 499,\n",
       " 1147,\n",
       " 252,\n",
       " 71,\n",
       " 281,\n",
       " 76,\n",
       " 688,\n",
       " 321,\n",
       " 63,\n",
       " 7,\n",
       " 242,\n",
       " 54,\n",
       " 562,\n",
       " 87,\n",
       " 1043,\n",
       " 929,\n",
       " 325,\n",
       " 615,\n",
       " 47,\n",
       " 608,\n",
       " 4,\n",
       " 376,\n",
       " 225,\n",
       " 396,\n",
       " 288,\n",
       " 204,\n",
       " 321,\n",
       " 75,\n",
       " 98,\n",
       " 259,\n",
       " 15,\n",
       " 575,\n",
       " 11,\n",
       " 51,\n",
       " 290,\n",
       " 418,\n",
       " 143,\n",
       " 294,\n",
       " 208,\n",
       " 322,\n",
       " 72,\n",
       " 35,\n",
       " 438,\n",
       " 264,\n",
       " 20,\n",
       " 205,\n",
       " 593,\n",
       " 276,\n",
       " 33,\n",
       " 99,\n",
       " 554,\n",
       " 724,\n",
       " 502,\n",
       " 184,\n",
       " 596,\n",
       " 1065,\n",
       " 314,\n",
       " 515,\n",
       " 270,\n",
       " 66,\n",
       " 485,\n",
       " 216,\n",
       " 426,\n",
       " 495,\n",
       " 82,\n",
       " 35,\n",
       " 1248,\n",
       " 734,\n",
       " 631,\n",
       " 107,\n",
       " 151,\n",
       " 28,\n",
       " 278,\n",
       " 872,\n",
       " 375,\n",
       " 93,\n",
       " 369,\n",
       " 439,\n",
       " 89,\n",
       " 553,\n",
       " 19,\n",
       " 422,\n",
       " 505,\n",
       " 465,\n",
       " 383,\n",
       " 502,\n",
       " 292,\n",
       " 152,\n",
       " 360,\n",
       " 637,\n",
       " 690,\n",
       " 358,\n",
       " 601,\n",
       " 385,\n",
       " 260,\n",
       " 124,\n",
       " 267,\n",
       " 405,\n",
       " 209,\n",
       " 334,\n",
       " 552,\n",
       " 371,\n",
       " 117,\n",
       " 554,\n",
       " 241,\n",
       " 3,\n",
       " 789,\n",
       " 613,\n",
       " 319,\n",
       " 574,\n",
       " 545,\n",
       " 367,\n",
       " 320,\n",
       " 1239,\n",
       " 404,\n",
       " 538,\n",
       " 183,\n",
       " 155,\n",
       " 328,\n",
       " 148,\n",
       " 227,\n",
       " 0,\n",
       " 38,\n",
       " 437,\n",
       " 319,\n",
       " 656,\n",
       " 456,\n",
       " 158,\n",
       " 37,\n",
       " 755,\n",
       " 347,\n",
       " 286,\n",
       " 133,\n",
       " 384,\n",
       " 219,\n",
       " 333,\n",
       " 550,\n",
       " 1052,\n",
       " 214,\n",
       " 339,\n",
       " 394,\n",
       " 107,\n",
       " 589,\n",
       " 0,\n",
       " 18,\n",
       " 325,\n",
       " 275,\n",
       " 237,\n",
       " 1185,\n",
       " 621,\n",
       " 242,\n",
       " 783,\n",
       " 21,\n",
       " 380,\n",
       " 274,\n",
       " 419,\n",
       " 137,\n",
       " 102,\n",
       " 108,\n",
       " 227,\n",
       " 522,\n",
       " 154,\n",
       " 43,\n",
       " 185,\n",
       " 52,\n",
       " 284,\n",
       " 459,\n",
       " 111,\n",
       " 216,\n",
       " 220,\n",
       " 652,\n",
       " 55,\n",
       " 688,\n",
       " 346,\n",
       " 364,\n",
       " 53,\n",
       " 0,\n",
       " 248,\n",
       " 84,\n",
       " 432,\n",
       " 3,\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset['start_positions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "301cc9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['index', 'id', 'context', 'question', 'answer_text', 'answer_start', 'input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 67801\n",
       "})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8b3f8196695ff2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:11:09.371618800Z",
     "start_time": "2023-10-22T19:11:09.257661100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_train_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    ")\n",
    "tf_val_dataset = model.prepare_tf_dataset(\n",
    "    tokenized_val_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b3fd7819ecf5aa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:11:10.718737Z",
     "start_time": "2023-10-22T19:11:10.693627900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(8, 384), dtype=tf.int64, name=None), 'token_type_ids': TensorSpec(shape=(8, 384), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(8, 384), dtype=tf.int64, name=None)}, {'start_positions': TensorSpec(shape=(8,), dtype=tf.int64, name=None), 'end_positions': TensorSpec(shape=(8,), dtype=tf.int64, name=None)})>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20ce86d0e18bd573",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# <_PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(16, 384), dtype=tf.int64, name=None), 'token_type_ids': TensorSpec(shape=(16, 384), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(16, 384), dtype=tf.int64, name=None)}, {'start_positions': TensorSpec(shape=(16,), dtype=tf.int64, name=None), 'end_positions': TensorSpec(shape=(16,), dtype=tf.int64, name=None)})>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9266b102c3f216",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:05:15.749790200Z",
     "start_time": "2023-10-22T19:05:15.666693500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_train_dataset = tokenized_train_dataset.to_tf_dataset(\n",
    "    columns=['input_ids', 'token_type_ids', 'attention_mask'],\n",
    "    label_cols=['start_positions', 'end_positions'],\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "tf_val_dataset = tokenized_val_dataset.to_tf_dataset(\n",
    "    columns=['input_ids', 'token_type_ids', 'attention_mask'],\n",
    "    label_cols=['start_positions', 'end_positions'],\n",
    "    batch_size=8,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e85759555d014d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:05:21.415558300Z",
     "start_time": "2023-10-22T19:05:21.362024100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 384), dtype=tf.int64, name=None), 'token_type_ids': TensorSpec(shape=(None, 384), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, 384), dtype=tf.int64, name=None)}, {'start_positions': TensorSpec(shape=(None,), dtype=tf.int64, name=None), 'end_positions': TensorSpec(shape=(None,), dtype=tf.int64, name=None)})>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f4ec7915adf3b24c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:11:14.571682100Z",
     "start_time": "2023-10-22T19:11:14.551558800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_train_epochs = 3\n",
    "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d3169885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Along with the United Democratic Party, what party currently rules the Marshall Islands?'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"question\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "71609d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Legislative power lies with the Nitijela. The upper house of Parliament, called the Council of Iroij, is an advisory body comprising twelve tribal chiefs. The executive branch consists of the President and the Presidential Cabinet, which consists of ten ministers appointed by the President with the approval of the Nitijela. The twenty-four electoral districts into which the country is divided correspond to the inhabited islands and atolls. There are currently four political parties in the Marshall Islands: Aelon̄ Kein Ad (AKA), United People's Party (UPP), Kien Eo Am (KEA) and United Democratic Party (UDP). Rule is shared by the AKA and the UDP. The following senators are in the legislative body:\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"context\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2ac9490f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test = tokenizer(train_dataset[\"question\"][0], train_dataset[\"context\"][0], return_tensors=\"tf\")\n",
    "output = model(**input_test)\n",
    "\n",
    "answer_start_index = int(tf.math.argmax(output.start_logits, axis=-1)[0])\n",
    "answer_end_index = int(tf.math.argmax(output.end_logits, axis=-1)[0])\n",
    "\n",
    "predict_answer_tokens = input_test.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9d0319c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test = question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "\n",
    "input_test = tokenizer(question, text, return_tensors=\"tf\")\n",
    "output = model(**input_test)\n",
    "\n",
    "answer_start_index = int(tf.math.argmax(output.start_logits, axis=-1)[0])\n",
    "answer_end_index = int(tf.math.argmax(output.end_logits, axis=-1)[0])\n",
    "\n",
    "predict_answer_tokens = input_test.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "751915ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=\n",
       "array([[  101,  2627,  1108,  3104,  1124, 15703,   136,   102,  3104,\n",
       "         1124, 15703,  1108,   170,  3505, 16797,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]])>, 'attention_mask': <tf.Tensor: shape=(1, 16), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "857e57fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFQuestionAnsweringModelOutput(loss=None, start_logits=<tf.Tensor: shape=(1, 16), dtype=float16, numpy=\n",
       "array([[-0.3826 , -0.02153, -0.414  , -0.1685 ,  0.11456, -0.08124,\n",
       "         0.0786 , -0.5254 , -0.00936,  0.328  ,  0.1414 , -0.273  ,\n",
       "        -0.1273 , -0.1489 , -0.2788 , -0.5254 ]], dtype=float16)>, end_logits=<tf.Tensor: shape=(1, 16), dtype=float16, numpy=\n",
       "array([[ 0.7124 , -0.0422 , -0.367  , -0.1194 ,  0.09766, -0.01522,\n",
       "         0.1018 , -0.2605 , -0.09937, -0.1198 , -0.2312 , -0.1819 ,\n",
       "        -0.3196 , -0.1366 ,  0.06235, -0.2605 ]], dtype=float16)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "834832c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'categorical_crossentropy'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "082a376d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.bert.modeling_tf_bert.TFBertForQuestionAnswering at 0x1e93d01b460>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "943d016bd9af345c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T19:11:32.995478700Z",
     "start_time": "2023-10-22T19:11:15.542622600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5170/8475 [=================>............] - ETA: 9:42 - loss: nan"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\STUDIA\\IPS\\question-answering\\medical-qa\\notebooks\\squad_bert.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/squad_bert.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(tf_train_dataset, validation_data\u001b[39m=\u001b[39;49mtf_val_dataset, epochs\u001b[39m=\u001b[39;49mnum_train_epochs)\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_val_dataset, epochs=num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "46ed3bc04bffb6bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T18:33:36.847962500Z",
     "start_time": "2023-10-22T18:33:36.809822400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_question_answering\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108891648 \n",
      "_________________________________________________________________\n",
      "qa_outputs (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 108,893,186\n",
      "Trainable params: 108,893,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722438ab47c3a76b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
