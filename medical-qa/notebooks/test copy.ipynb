{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering, DefaultDataCollator, create_optimizer\n",
    "import datasets\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\pyarrow\\pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "def load_train_val_test_datasets(dataset_path='./../data/datasets/squad'):\n",
    "    train = pd.read_csv(f'{dataset_path}/train.csv').dropna()\n",
    "    val = pd.read_csv(f'{dataset_path}/dev.csv').dropna()\n",
    "    test = pd.read_csv(f'{dataset_path}/test.csv').dropna()\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def convert_dataframes_to_datasets(dataframes: list):\n",
    "    return tuple(\n",
    "        [datasets.Dataset.from_pandas(dataframe, preserve_index=False) for dataframe in\n",
    "         dataframes])\n",
    "\n",
    "\n",
    "df_train, df_val, df_test = load_train_val_test_datasets()\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = convert_dataframes_to_datasets([df_train, df_val, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = datasets.DatasetDict({\"train\":train_dataset, \"validation\":val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'id', 'context', 'question', 'answer_text', 'answer_start'],\n",
       "        num_rows: 68716\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['index', 'id', 'context', 'question', 'answer_text', 'answer_start'],\n",
       "        num_rows: 14724\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# raw_datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answer_texts = examples[\"answer_text\"]\n",
    "    answer_starts = examples[\"answer_start\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer_text = answer_texts[sample_idx]\n",
    "        start_char = answer_starts[sample_idx]\n",
    "        end_char = start_char + len(answer_text)\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f074fe755a84aa286253de76a203dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/68716 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(68716, 69689)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5574a79cf0c47b1b5ad25fd21abc036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(14724, 14916)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    train_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=4,\n",
    ")\n",
    "tf_eval_dataset = model.prepare_tf_dataset(\n",
    "    validation_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4070, compute capability 8.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
    "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
    "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
    "num_train_epochs = 3\n",
    "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'tf_bert_for_question_answering/bert/encoder/layer_._3/attention/self/dropout_10/dropout/random_uniform/RandomUniform' defined at (most recent call last):\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Artur\\AppData\\Local\\Temp\\ipykernel_21120\\2962994209.py\", line 2, in <module>\n      model.fit(tf_train_dataset, epochs=num_train_epochs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1637, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1833, in run_call_with_unpacked_inputs\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1852, in call\n      outputs = self.bert(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1833, in run_call_with_unpacked_inputs\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 862, in call\n      encoder_outputs = self.encoder(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 548, in call\n      for i, layer_module in enumerate(self.layer):\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 554, in call\n      layer_outputs = layer_module(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 464, in call\n      self_attention_outputs = self.attention(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 380, in call\n      self_outputs = self.self_attention(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 323, in call\n      attention_probs = self.dropout(inputs=attention_probs, training=training)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 116, in call\n      output = control_flow_util.smart_cond(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 112, in dropped_inputs\n      return self._random_generator.dropout(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\backend.py\", line 2162, in dropout\n      return tf.nn.dropout(\nNode: 'tf_bert_for_question_answering/bert/encoder/layer_._3/attention/self/dropout_10/dropout/random_uniform/RandomUniform'\nOOM when allocating tensor with shape[4,12,384,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node tf_bert_for_question_answering/bert/encoder/layer_._3/attention/self/dropout_10/dropout/random_uniform/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_17425]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32me:\\STUDIA\\IPS\\question-answering\\medical-qa\\notebooks\\test copy.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/test%20copy.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# We're going to do validation afterwards, so no validation mid-training\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/test%20copy.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(tf_train_dataset, epochs\u001b[39m=\u001b[39;49mnum_train_epochs)\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'tf_bert_for_question_answering/bert/encoder/layer_._3/attention/self/dropout_10/dropout/random_uniform/RandomUniform' defined at (most recent call last):\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Artur\\AppData\\Local\\Temp\\ipykernel_21120\\2962994209.py\", line 2, in <module>\n      model.fit(tf_train_dataset, epochs=num_train_epochs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1637, in train_step\n      y_pred = self(x, training=True)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1833, in run_call_with_unpacked_inputs\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 1852, in call\n      outputs = self.bert(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1833, in run_call_with_unpacked_inputs\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 862, in call\n      encoder_outputs = self.encoder(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 548, in call\n      for i, layer_module in enumerate(self.layer):\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 554, in call\n      layer_outputs = layer_module(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 464, in call\n      self_attention_outputs = self.attention(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 380, in call\n      self_outputs = self.self_attention(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py\", line 323, in call\n      attention_probs = self.dropout(inputs=attention_probs, training=training)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 116, in call\n      output = control_flow_util.smart_cond(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 112, in dropped_inputs\n      return self._random_generator.dropout(\n    File \"c:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\backend.py\", line 2162, in dropout\n      return tf.nn.dropout(\nNode: 'tf_bert_for_question_answering/bert/encoder/layer_._3/attention/self/dropout_10/dropout/random_uniform/RandomUniform'\nOOM when allocating tensor with shape[4,12,384,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node tf_bert_for_question_answering/bert/encoder/layer_._3/attention/self/dropout_10/dropout/random_uniform/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_17425]"
     ]
    }
   ],
   "source": [
    "# We're going to do validation afterwards, so no validation mid-training\n",
    "model.fit(tf_train_dataset, epochs=num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
