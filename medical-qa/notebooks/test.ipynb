{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T19:49:48.656182400Z",
     "start_time": "2023-10-24T19:49:44.615652500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering, DefaultDataCollator, create_optimizer\n",
    "import datasets\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T19:49:51.181237400Z",
     "start_time": "2023-10-24T19:49:48.660208700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (C:/Users/Karol/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26087b98e9614b0d87c895a607d40190"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T19:49:51.203037Z",
     "start_time": "2023-10-24T19:49:51.184239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 87599\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 10570\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T19:49:51.400399800Z",
     "start_time": "2023-10-24T19:49:51.199037600Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T19:49:51.420419100Z",
     "start_time": "2023-10-24T19:49:51.403402600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['id', 'title', 'context', 'question', 'answers'],\n    num_rows: 87599\n})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T19:49:52.568307300Z",
     "start_time": "2023-10-24T19:49:51.417418100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/87599 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a38bc237e4c64c8e82fc5421e7bbe0be"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'Along with the United Democratic Party, what party currently rules the Marshall Islands?'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = raw_datasets['train'].filter(lambda x: x['id'] == '56f961049b226e1400dd13eb')\n",
    "a['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T19:49:52.610698100Z",
     "start_time": "2023-10-24T19:49:52.566300200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "633"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_char = a['answers'][0]['answer_start'][0]\n",
    "start_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-24T19:49:52.610698100Z",
     "start_time": "2023-10-24T19:49:52.581825700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "640"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_char = start_char + len(a['answers'][0]['text'][0])\n",
    "end_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "question = a['question'][0]\n",
    "context = a['context'][0]\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=max_length,\n",
    "    padding='max_length',\n",
    "    truncation=\"only_second\",\n",
    "    stride=stride,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "# inputs.sequence_ids(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T19:58:00.368239300Z",
     "start_time": "2023-10-24T19:58:00.336126800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43minputs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msequence_ids\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ml-tf2gpu\\lib\\site-packages\\transformers\\tokenization_utils_base.py:323\u001B[0m, in \u001B[0;36mBatchEncoding.sequence_ids\u001B[1;34m(self, batch_index)\u001B[0m\n\u001B[0;32m    318\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_encodings:\n\u001B[0;32m    319\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msequence_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m class).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n\u001B[1;32m--> 323\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_encodings\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbatch_index\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39msequence_ids\n",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "inputs.sequence_ids(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T19:58:33.144357800Z",
     "start_time": "2023-10-24T19:58:32.486112200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[[(0, 0),\n  (0, 5),\n  (6, 10),\n  (11, 14),\n  (15, 21),\n  (22, 32),\n  (33, 38),\n  (38, 39),\n  (40, 44),\n  (45, 50),\n  (51, 60),\n  (61, 66),\n  (67, 70),\n  (71, 79),\n  (80, 87),\n  (87, 88),\n  (0, 0),\n  (0, 11),\n  (12, 17),\n  (18, 22),\n  (23, 27),\n  (28, 31),\n  (32, 34),\n  (34, 36),\n  (36, 38),\n  (38, 40),\n  (40, 41),\n  (42, 45),\n  (46, 51),\n  (52, 57),\n  (58, 60),\n  (61, 71),\n  (71, 72),\n  (73, 79),\n  (80, 83),\n  (84, 91),\n  (92, 94),\n  (95, 96),\n  (96, 99),\n  (99, 100),\n  (100, 101),\n  (102, 104),\n  (105, 107),\n  (108, 116),\n  (117, 121),\n  (122, 132),\n  (133, 139),\n  (140, 146),\n  (147, 153),\n  (153, 154),\n  (155, 158),\n  (159, 168),\n  (169, 175),\n  (176, 184),\n  (185, 187),\n  (188, 191),\n  (192, 201),\n  (202, 205),\n  (206, 209),\n  (210, 222),\n  (223, 230),\n  (230, 231),\n  (232, 237),\n  (238, 246),\n  (247, 249),\n  (250, 253),\n  (254, 263),\n  (264, 273),\n  (274, 276),\n  (277, 280),\n  (281, 290),\n  (291, 295),\n  (296, 299),\n  (300, 308),\n  (309, 311),\n  (312, 315),\n  (316, 318),\n  (318, 320),\n  (320, 322),\n  (322, 324),\n  (324, 325),\n  (326, 329),\n  (330, 336),\n  (336, 337),\n  (337, 341),\n  (342, 351),\n  (352, 361),\n  (362, 366),\n  (367, 372),\n  (373, 376),\n  (377, 384),\n  (385, 387),\n  (388, 395),\n  (396, 406),\n  (407, 409),\n  (410, 413),\n  (414, 423),\n  (424, 431),\n  (432, 435),\n  (436, 438),\n  (438, 441),\n  (441, 442),\n  (442, 443),\n  (444, 449),\n  (450, 453),\n  (454, 463),\n  (464, 468),\n  (469, 478),\n  (479, 486),\n  (487, 489),\n  (490, 493),\n  (494, 502),\n  (503, 510),\n  (510, 511),\n  (512, 518),\n  (519, 521),\n  (521, 523),\n  (524, 526),\n  (527, 528),\n  (528, 530),\n  (530, 531),\n  (531, 532),\n  (532, 533),\n  (534, 540),\n  (541, 547),\n  (547, 548),\n  (548, 549),\n  (550, 555),\n  (556, 557),\n  (557, 559),\n  (559, 560),\n  (560, 561),\n  (561, 562),\n  (563, 565),\n  (565, 567),\n  (568, 569),\n  (569, 570),\n  (571, 573),\n  (574, 575),\n  (575, 576),\n  (576, 578),\n  (578, 579),\n  (580, 583),\n  (584, 590),\n  (591, 601),\n  (602, 607),\n  (608, 609),\n  (609, 610),\n  (610, 612),\n  (612, 613),\n  (613, 614),\n  (615, 619),\n  (620, 622),\n  (623, 629),\n  (630, 632),\n  (633, 636),\n  (637, 639),\n  (639, 640),\n  (641, 644),\n  (645, 648),\n  (649, 650),\n  (650, 652),\n  (652, 653),\n  (654, 657),\n  (658, 667),\n  (668, 676),\n  (677, 680),\n  (681, 683),\n  (684, 687),\n  (688, 699),\n  (700, 704),\n  (704, 705),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0),\n  (0, 0)]]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.pop('offset_mapping')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-24T19:56:12.374204400Z",
     "start_time": "2023-10-24T19:56:12.306010500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Artur\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-ccda88a6d5c6db79.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(87599, 88729)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 88729\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Artur\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-0002ea78c210be2c.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10570, 10822)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    train_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    ")\n",
    "tf_eval_dataset = model.prepare_tf_dataset(\n",
    "    validation_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4070, compute capability 8.9\n"
     ]
    }
   ],
   "source": [
    "from transformers import create_optimizer\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "import tensorflow as tf\n",
    "\n",
    "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
    "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
    "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
    "num_train_epochs = 3\n",
    "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "   78/11091 [..............................] - ETA: 41:25 - loss: 4.4157"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32me:\\STUDIA\\IPS\\question-answering\\medical-qa\\notebooks\\test.ipynb Cell 12\u001B[0m line \u001B[0;36m4\n\u001B[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/test.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39mtransformers\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mkeras_callbacks\u001B[39;00m \u001B[39mimport\u001B[39;00m PushToHubCallback\n\u001B[0;32m      <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/test.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001B[0m \u001B[39m# We're going to do validation afterwards, so no validation mid-training\u001B[39;00m\n\u001B[1;32m----> <a href='vscode-notebook-cell:/e%3A/STUDIA/IPS/question-answering/medical-qa/notebooks/test.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001B[0m model\u001B[39m.\u001B[39;49mfit(tf_train_dataset, epochs\u001B[39m=\u001B[39;49mnum_train_epochs)\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[39m=\u001B[39m \u001B[39mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[39mreturn\u001B[39;00m fn(\u001B[39m*\u001B[39margs, \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mException\u001B[39;00m \u001B[39mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[39m=\u001B[39m _process_traceback_frames(e\u001B[39m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\engine\\training.py:1570\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1568\u001B[0m logs \u001B[39m=\u001B[39m tmp_logs\n\u001B[0;32m   1569\u001B[0m end_step \u001B[39m=\u001B[39m step \u001B[39m+\u001B[39m data_handler\u001B[39m.\u001B[39mstep_increment\n\u001B[1;32m-> 1570\u001B[0m callbacks\u001B[39m.\u001B[39;49mon_train_batch_end(end_step, logs)\n\u001B[0;32m   1571\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mstop_training:\n\u001B[0;32m   1572\u001B[0m     \u001B[39mbreak\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\callbacks.py:470\u001B[0m, in \u001B[0;36mCallbackList.on_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m    463\u001B[0m \u001B[39m\u001B[39m\u001B[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001B[39;00m\n\u001B[0;32m    464\u001B[0m \n\u001B[0;32m    465\u001B[0m \u001B[39mArgs:\u001B[39;00m\n\u001B[0;32m    466\u001B[0m \u001B[39m    batch: Integer, index of batch within the current epoch.\u001B[39;00m\n\u001B[0;32m    467\u001B[0m \u001B[39m    logs: Dict. Aggregated metric results up until this batch.\u001B[39;00m\n\u001B[0;32m    468\u001B[0m \u001B[39m\"\"\"\u001B[39;00m\n\u001B[0;32m    469\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_should_call_train_batch_hooks:\n\u001B[1;32m--> 470\u001B[0m     \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_call_batch_hook(ModeKeys\u001B[39m.\u001B[39;49mTRAIN, \u001B[39m\"\u001B[39;49m\u001B[39mend\u001B[39;49m\u001B[39m\"\u001B[39;49m, batch, logs\u001B[39m=\u001B[39;49mlogs)\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\callbacks.py:317\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook\u001B[1;34m(self, mode, hook, batch, logs)\u001B[0m\n\u001B[0;32m    315\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_call_batch_begin_hook(mode, batch, logs)\n\u001B[0;32m    316\u001B[0m \u001B[39melif\u001B[39;00m hook \u001B[39m==\u001B[39m \u001B[39m\"\u001B[39m\u001B[39mend\u001B[39m\u001B[39m\"\u001B[39m:\n\u001B[1;32m--> 317\u001B[0m     \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001B[0;32m    318\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m    319\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\n\u001B[0;32m    320\u001B[0m         \u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mUnrecognized hook: \u001B[39m\u001B[39m{\u001B[39;00mhook\u001B[39m}\u001B[39;00m\u001B[39m. \u001B[39m\u001B[39m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[39m'\u001B[39m\u001B[39mExpected values are [\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mbegin\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m, \u001B[39m\u001B[39m\"\u001B[39m\u001B[39mend\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m]\u001B[39m\u001B[39m'\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\callbacks.py:340\u001B[0m, in \u001B[0;36mCallbackList._call_batch_end_hook\u001B[1;34m(self, mode, batch, logs)\u001B[0m\n\u001B[0;32m    337\u001B[0m     batch_time \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime() \u001B[39m-\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_batch_start_time\n\u001B[0;32m    338\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_batch_times\u001B[39m.\u001B[39mappend(batch_time)\n\u001B[1;32m--> 340\u001B[0m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001B[0;32m    342\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mlen\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_batch_times) \u001B[39m>\u001B[39m\u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_num_batches_for_timing_check:\n\u001B[0;32m    343\u001B[0m     end_hook_name \u001B[39m=\u001B[39m hook_name\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\callbacks.py:388\u001B[0m, in \u001B[0;36mCallbackList._call_batch_hook_helper\u001B[1;34m(self, hook_name, batch, logs)\u001B[0m\n\u001B[0;32m    386\u001B[0m \u001B[39mfor\u001B[39;00m callback \u001B[39min\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mcallbacks:\n\u001B[0;32m    387\u001B[0m     hook \u001B[39m=\u001B[39m \u001B[39mgetattr\u001B[39m(callback, hook_name)\n\u001B[1;32m--> 388\u001B[0m     hook(batch, logs)\n\u001B[0;32m    390\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_check_timing:\n\u001B[0;32m    391\u001B[0m     \u001B[39mif\u001B[39;00m hook_name \u001B[39mnot\u001B[39;00m \u001B[39min\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_hook_times:\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\callbacks.py:1081\u001B[0m, in \u001B[0;36mProgbarLogger.on_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m   1080\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mon_train_batch_end\u001B[39m(\u001B[39mself\u001B[39m, batch, logs\u001B[39m=\u001B[39m\u001B[39mNone\u001B[39;00m):\n\u001B[1;32m-> 1081\u001B[0m     \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\callbacks.py:1157\u001B[0m, in \u001B[0;36mProgbarLogger._batch_update_progbar\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m   1153\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mseen \u001B[39m+\u001B[39m\u001B[39m=\u001B[39m add_seen\n\u001B[0;32m   1155\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mverbose \u001B[39m==\u001B[39m \u001B[39m1\u001B[39m:\n\u001B[0;32m   1156\u001B[0m     \u001B[39m# Only block async when verbose = 1.\u001B[39;00m\n\u001B[1;32m-> 1157\u001B[0m     logs \u001B[39m=\u001B[39m tf_utils\u001B[39m.\u001B[39;49msync_to_numpy_or_python_type(logs)\n\u001B[0;32m   1158\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mprogbar\u001B[39m.\u001B[39mupdate(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39mseen, \u001B[39mlist\u001B[39m(logs\u001B[39m.\u001B[39mitems()), finalize\u001B[39m=\u001B[39m\u001B[39mFalse\u001B[39;00m)\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type\u001B[1;34m(tensors)\u001B[0m\n\u001B[0;32m    632\u001B[0m         \u001B[39mreturn\u001B[39;00m t\n\u001B[0;32m    633\u001B[0m     \u001B[39mreturn\u001B[39;00m t\u001B[39m.\u001B[39mitem() \u001B[39mif\u001B[39;00m np\u001B[39m.\u001B[39mndim(t) \u001B[39m==\u001B[39m \u001B[39m0\u001B[39m \u001B[39melse\u001B[39;00m t\n\u001B[1;32m--> 635\u001B[0m \u001B[39mreturn\u001B[39;00m tf\u001B[39m.\u001B[39;49mnest\u001B[39m.\u001B[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001B[0m, in \u001B[0;36mmap_structure\u001B[1;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[0;32m    913\u001B[0m flat_structure \u001B[39m=\u001B[39m (flatten(s, expand_composites) \u001B[39mfor\u001B[39;00m s \u001B[39min\u001B[39;00m structure)\n\u001B[0;32m    914\u001B[0m entries \u001B[39m=\u001B[39m \u001B[39mzip\u001B[39m(\u001B[39m*\u001B[39mflat_structure)\n\u001B[0;32m    916\u001B[0m \u001B[39mreturn\u001B[39;00m pack_sequence_as(\n\u001B[1;32m--> 917\u001B[0m     structure[\u001B[39m0\u001B[39m], [func(\u001B[39m*\u001B[39mx) \u001B[39mfor\u001B[39;00m x \u001B[39min\u001B[39;00m entries],\n\u001B[0;32m    918\u001B[0m     expand_composites\u001B[39m=\u001B[39mexpand_composites)\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    913\u001B[0m flat_structure \u001B[39m=\u001B[39m (flatten(s, expand_composites) \u001B[39mfor\u001B[39;00m s \u001B[39min\u001B[39;00m structure)\n\u001B[0;32m    914\u001B[0m entries \u001B[39m=\u001B[39m \u001B[39mzip\u001B[39m(\u001B[39m*\u001B[39mflat_structure)\n\u001B[0;32m    916\u001B[0m \u001B[39mreturn\u001B[39;00m pack_sequence_as(\n\u001B[1;32m--> 917\u001B[0m     structure[\u001B[39m0\u001B[39m], [func(\u001B[39m*\u001B[39;49mx) \u001B[39mfor\u001B[39;00m x \u001B[39min\u001B[39;00m entries],\n\u001B[0;32m    918\u001B[0m     expand_composites\u001B[39m=\u001B[39mexpand_composites)\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001B[0m, in \u001B[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    625\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_to_single_numpy_or_python_type\u001B[39m(t):\n\u001B[0;32m    626\u001B[0m     \u001B[39m# Don't turn ragged or sparse tensors to NumPy.\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39misinstance\u001B[39m(t, tf\u001B[39m.\u001B[39mTensor):\n\u001B[1;32m--> 628\u001B[0m         t \u001B[39m=\u001B[39m t\u001B[39m.\u001B[39;49mnumpy()\n\u001B[0;32m    629\u001B[0m     \u001B[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[39m# as-is.\u001B[39;00m\n\u001B[0;32m    631\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39misinstance\u001B[39m(t, (np\u001B[39m.\u001B[39mndarray, np\u001B[39m.\u001B[39mgeneric)):\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001B[0m, in \u001B[0;36m_EagerTensorBase.numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1134\u001B[0m \u001B[39m\u001B[39m\u001B[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001B[39;00m\n\u001B[0;32m   1135\u001B[0m \n\u001B[0;32m   1136\u001B[0m \u001B[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1154\u001B[0m \u001B[39m    NumPy dtype.\u001B[39;00m\n\u001B[0;32m   1155\u001B[0m \u001B[39m\"\"\"\u001B[39;00m\n\u001B[0;32m   1156\u001B[0m \u001B[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001B[39;00m\n\u001B[1;32m-> 1157\u001B[0m maybe_arr \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_numpy()  \u001B[39m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   1158\u001B[0m \u001B[39mreturn\u001B[39;00m maybe_arr\u001B[39m.\u001B[39mcopy() \u001B[39mif\u001B[39;00m \u001B[39misinstance\u001B[39m(maybe_arr, np\u001B[39m.\u001B[39mndarray) \u001B[39melse\u001B[39;00m maybe_arr\n",
      "File \u001B[1;32mc:\\Users\\Artur\\.conda\\envs\\nlp_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001B[0m, in \u001B[0;36m_EagerTensorBase._numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1121\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_numpy\u001B[39m(\u001B[39mself\u001B[39m):\n\u001B[0;32m   1122\u001B[0m   \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m-> 1123\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_numpy_internal()\n\u001B[0;32m   1124\u001B[0m   \u001B[39mexcept\u001B[39;00m core\u001B[39m.\u001B[39m_NotOkStatusException \u001B[39mas\u001B[39;00m e:  \u001B[39m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   1125\u001B[0m     \u001B[39mraise\u001B[39;00m core\u001B[39m.\u001B[39m_status_to_exception(e) \u001B[39mfrom\u001B[39;00m \u001B[39mNone\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "# We're going to do validation afterwards, so no validation mid-training\n",
    "model.fit(tf_train_dataset, epochs=num_train_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
